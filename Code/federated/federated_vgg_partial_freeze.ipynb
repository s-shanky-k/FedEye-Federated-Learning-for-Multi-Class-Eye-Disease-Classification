{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40deff56",
   "metadata": {
    "id": "cz71fPGrpRiQ",
    "papermill": {
     "duration": 0.008064,
     "end_time": "2023-12-12T01:45:47.277186",
     "exception": false,
     "start_time": "2023-12-12T01:45:47.269122",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Federated Learning In ResNet - Eye Disease Classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcaf122",
   "metadata": {
    "id": "D4KiTMTpiort",
    "papermill": {
     "duration": 0.007279,
     "end_time": "2023-12-12T01:45:47.292059",
     "exception": false,
     "start_time": "2023-12-12T01:45:47.284780",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Installing dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4004a7c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T01:45:47.308564Z",
     "iopub.status.busy": "2023-12-12T01:45:47.308206Z",
     "iopub.status.idle": "2023-12-12T01:46:11.645871Z",
     "shell.execute_reply": "2023-12-12T01:46:11.644761Z"
    },
    "id": "9PAxbNFt6in6",
    "papermill": {
     "duration": 24.348836,
     "end_time": "2023-12-12T01:46:11.648268",
     "exception": false,
     "start_time": "2023-12-12T01:45:47.299432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flwr[simulation]==0.18.0\r\n",
      "  Downloading flwr-0.18.0-py3-none-any.whl (106 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.2/106.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.0)\r\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.15.1)\r\n",
      "Collecting google<3.0.0,>=2.0.3 (from flwr[simulation]==0.18.0)\r\n",
      "  Downloading google-2.0.3-py2.py3-none-any.whl (45 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting grpcio<=1.43.0,>=1.27.2 (from flwr[simulation]==0.18.0)\r\n",
      "  Downloading grpcio-1.43.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from flwr[simulation]==0.18.0) (1.24.3)\r\n",
      "Requirement already satisfied: protobuf<4.0.0,>=3.12.1 in /opt/conda/lib/python3.10/site-packages (from flwr[simulation]==0.18.0) (3.20.3)\r\n",
      "Collecting ray[default]<2.0.0,>=1.9.2 (from flwr[simulation]==0.18.0)\r\n",
      "  Downloading ray-1.13.0-cp310-cp310-manylinux2014_x86_64.whl (54.3 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.2)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.5.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.31.0)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (10.1.0)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from google<3.0.0,>=2.0.3->flwr[simulation]==0.18.0) (4.12.2)\r\n",
      "Requirement already satisfied: six>=1.5.2 in /opt/conda/lib/python3.10/site-packages (from grpcio<=1.43.0,>=1.27.2->flwr[simulation]==0.18.0) (1.16.0)\r\n",
      "Requirement already satisfied: attrs in /opt/conda/lib/python3.10/site-packages (from ray[default]<2.0.0,>=1.9.2->flwr[simulation]==0.18.0) (23.1.0)\r\n",
      "Collecting click<=8.0.4,>=7.0 (from ray[default]<2.0.0,>=1.9.2->flwr[simulation]==0.18.0)\r\n",
      "  Downloading click-8.0.4-py3-none-any.whl (97 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.5/97.5 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from ray[default]<2.0.0,>=1.9.2->flwr[simulation]==0.18.0) (4.19.0)\r\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ray[default]<2.0.0,>=1.9.2->flwr[simulation]==0.18.0) (1.0.5)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from ray[default]<2.0.0,>=1.9.2->flwr[simulation]==0.18.0) (6.0.1)\r\n",
      "Requirement already satisfied: aiosignal in /opt/conda/lib/python3.10/site-packages (from ray[default]<2.0.0,>=1.9.2->flwr[simulation]==0.18.0) (1.3.1)\r\n",
      "Requirement already satisfied: frozenlist in /opt/conda/lib/python3.10/site-packages (from ray[default]<2.0.0,>=1.9.2->flwr[simulation]==0.18.0) (1.4.0)\r\n",
      "Requirement already satisfied: virtualenv in /opt/conda/lib/python3.10/site-packages (from ray[default]<2.0.0,>=1.9.2->flwr[simulation]==0.18.0) (20.21.0)\r\n",
      "Requirement already satisfied: aiohttp>=3.7 in /opt/conda/lib/python3.10/site-packages (from ray[default]<2.0.0,>=1.9.2->flwr[simulation]==0.18.0) (3.8.5)\r\n",
      "Requirement already satisfied: aiohttp-cors in /opt/conda/lib/python3.10/site-packages (from ray[default]<2.0.0,>=1.9.2->flwr[simulation]==0.18.0) (0.7.0)\r\n",
      "Requirement already satisfied: colorful in /opt/conda/lib/python3.10/site-packages (from ray[default]<2.0.0,>=1.9.2->flwr[simulation]==0.18.0) (0.5.5)\r\n",
      "Requirement already satisfied: py-spy>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from ray[default]<2.0.0,>=1.9.2->flwr[simulation]==0.18.0) (0.3.14)\r\n",
      "Requirement already satisfied: gpustat>=1.0.0b1 in /opt/conda/lib/python3.10/site-packages (from ray[default]<2.0.0,>=1.9.2->flwr[simulation]==0.18.0) (1.0.0)\r\n",
      "Requirement already satisfied: opencensus in /opt/conda/lib/python3.10/site-packages (from ray[default]<2.0.0,>=1.9.2->flwr[simulation]==0.18.0) (0.11.2)\r\n",
      "Collecting prometheus-client<0.14.0,>=0.7.1 (from ray[default]<2.0.0,>=1.9.2->flwr[simulation]==0.18.0)\r\n",
      "  Downloading prometheus_client-0.13.1-py3-none-any.whl (57 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: smart-open in /opt/conda/lib/python3.10/site-packages (from ray[default]<2.0.0,>=1.9.2->flwr[simulation]==0.18.0) (6.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.2.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2023.11.17)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp>=3.7->ray[default]<2.0.0,>=1.9.2->flwr[simulation]==0.18.0) (6.0.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp>=3.7->ray[default]<2.0.0,>=1.9.2->flwr[simulation]==0.18.0) (4.0.3)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp>=3.7->ray[default]<2.0.0,>=1.9.2->flwr[simulation]==0.18.0) (1.9.2)\r\n",
      "Requirement already satisfied: nvidia-ml-py<=11.495.46,>=11.450.129 in /opt/conda/lib/python3.10/site-packages (from gpustat>=1.0.0b1->ray[default]<2.0.0,>=1.9.2->flwr[simulation]==0.18.0) (11.495.46)\r\n",
      "Requirement already satisfied: psutil>=5.6.0 in /opt/conda/lib/python3.10/site-packages (from gpustat>=1.0.0b1->ray[default]<2.0.0,>=1.9.2->flwr[simulation]==0.18.0) (5.9.3)\r\n",
      "Requirement already satisfied: blessed>=1.17.1 in /opt/conda/lib/python3.10/site-packages (from gpustat>=1.0.0b1->ray[default]<2.0.0,>=1.9.2->flwr[simulation]==0.18.0) (1.20.0)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->google<3.0.0,>=2.0.3->flwr[simulation]==0.18.0) (2.3.2.post1)\r\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray[default]<2.0.0,>=1.9.2->flwr[simulation]==0.18.0) (2023.7.1)\r\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray[default]<2.0.0,>=1.9.2->flwr[simulation]==0.18.0) (0.30.2)\r\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray[default]<2.0.0,>=1.9.2->flwr[simulation]==0.18.0) (0.9.2)\r\n",
      "Requirement already satisfied: opencensus-context>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from opencensus->ray[default]<2.0.0,>=1.9.2->flwr[simulation]==0.18.0) (0.1.3)\r\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from opencensus->ray[default]<2.0.0,>=1.9.2->flwr[simulation]==0.18.0) (2.11.1)\r\n",
      "Requirement already satisfied: distlib<1,>=0.3.6 in /opt/conda/lib/python3.10/site-packages (from virtualenv->ray[default]<2.0.0,>=1.9.2->flwr[simulation]==0.18.0) (0.3.7)\r\n",
      "Collecting platformdirs<4,>=2.4 (from virtualenv->ray[default]<2.0.0,>=1.9.2->flwr[simulation]==0.18.0)\r\n",
      "  Obtaining dependency information for platformdirs<4,>=2.4 from https://files.pythonhosted.org/packages/56/29/3ec311dc18804409ecf0d2b09caa976f3ae6215559306b5b530004e11156/platformdirs-3.11.0-py3-none-any.whl.metadata\r\n",
      "  Downloading platformdirs-3.11.0-py3-none-any.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in /opt/conda/lib/python3.10/site-packages (from blessed>=1.17.1->gpustat>=1.0.0b1->ray[default]<2.0.0,>=1.9.2->flwr[simulation]==0.18.0) (0.2.6)\r\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]<2.0.0,>=1.9.2->flwr[simulation]==0.18.0) (1.60.0)\r\n",
      "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]<2.0.0,>=1.9.2->flwr[simulation]==0.18.0) (2.22.0)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]<2.0.0,>=1.9.2->flwr[simulation]==0.18.0) (4.2.4)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]<2.0.0,>=1.9.2->flwr[simulation]==0.18.0) (0.2.7)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]<2.0.0,>=1.9.2->flwr[simulation]==0.18.0) (4.9)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]<2.0.0,>=1.9.2->flwr[simulation]==0.18.0) (0.4.8)\r\n",
      "Downloading platformdirs-3.11.0-py3-none-any.whl (17 kB)\r\n",
      "Installing collected packages: prometheus-client, platformdirs, grpcio, click, google, flwr, ray\r\n",
      "  Attempting uninstall: prometheus-client\r\n",
      "    Found existing installation: prometheus-client 0.17.1\r\n",
      "    Uninstalling prometheus-client-0.17.1:\r\n",
      "      Successfully uninstalled prometheus-client-0.17.1\r\n",
      "  Attempting uninstall: platformdirs\r\n",
      "    Found existing installation: platformdirs 4.1.0\r\n",
      "    Uninstalling platformdirs-4.1.0:\r\n",
      "      Successfully uninstalled platformdirs-4.1.0\r\n",
      "  Attempting uninstall: grpcio\r\n",
      "    Found existing installation: grpcio 1.51.1\r\n",
      "    Uninstalling grpcio-1.51.1:\r\n",
      "      Successfully uninstalled grpcio-1.51.1\r\n",
      "  Attempting uninstall: click\r\n",
      "    Found existing installation: click 8.1.7\r\n",
      "    Uninstalling click-8.1.7:\r\n",
      "      Successfully uninstalled click-8.1.7\r\n",
      "  Attempting uninstall: ray\r\n",
      "    Found existing installation: ray 2.6.3\r\n",
      "    Uninstalling ray-2.6.3:\r\n",
      "      Successfully uninstalled ray-2.6.3\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\r\n",
      "dask 2023.12.0 requires click>=8.1, but you have click 8.0.4 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\r\n",
      "fitter 1.6.0 requires click<9.0.0,>=8.1.6, but you have click 8.0.4 which is incompatible.\r\n",
      "flask 3.0.0 requires click>=8.1.3, but you have click 8.0.4 which is incompatible.\r\n",
      "google-cloud-pubsub 2.18.3 requires grpcio<2.0dev,>=1.51.3, but you have grpcio 1.43.0 which is incompatible.\r\n",
      "grpc-google-iam-v1 0.12.6 requires grpcio<2.0.0dev,>=1.44.0, but you have grpcio 1.43.0 which is incompatible.\r\n",
      "grpcio-status 1.48.1 requires grpcio>=1.48.1, but you have grpcio 1.43.0 which is incompatible.\r\n",
      "jupyterlab 4.0.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n",
      "jupyterlab-lsp 5.0.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n",
      "jupyterlab-lsp 5.0.1 requires jupyterlab<5.0.0a0,>=4.0.6, but you have jupyterlab 4.0.5 which is incompatible.\r\n",
      "kfp 2.0.1 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\r\n",
      "libpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\r\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "raft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\r\n",
      "raft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\r\n",
      "tensorboard 2.13.0 requires grpcio>=1.48.2, but you have grpcio 1.43.0 which is incompatible.\r\n",
      "tensorflowjs 4.14.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed click-8.0.4 flwr-0.18.0 google-2.0.3 grpcio-1.43.0 platformdirs-3.11.0 prometheus-client-0.13.1 ray-1.13.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install flwr[\"simulation\"]==0.18.0 torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8a12cce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T01:46:11.677527Z",
     "iopub.status.busy": "2023-12-12T01:46:11.677195Z",
     "iopub.status.idle": "2023-12-12T01:46:15.984601Z",
     "shell.execute_reply": "2023-12-12T01:46:15.983367Z"
    },
    "id": "eTrCL2FmC5U5",
    "outputId": "35239cde-c6ea-4a2d-a458-c8ce7597c10c",
    "papermill": {
     "duration": 4.324551,
     "end_time": "2023-12-12T01:46:15.987270",
     "exception": false,
     "start_time": "2023-12-12T01:46:11.662719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import flwr as fl\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, utils\n",
    "torch.manual_seed(42)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4335a2e5",
   "metadata": {
    "id": "JVcgAAiaihnx",
    "papermill": {
     "duration": 0.013575,
     "end_time": "2023-12-12T01:46:16.018059",
     "exception": false,
     "start_time": "2023-12-12T01:46:16.004484",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b55a2249",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T01:46:16.047069Z",
     "iopub.status.busy": "2023-12-12T01:46:16.046528Z",
     "iopub.status.idle": "2023-12-12T01:46:16.053054Z",
     "shell.execute_reply": "2023-12-12T01:46:16.052174Z"
    },
    "papermill": {
     "duration": 0.022838,
     "end_time": "2023-12-12T01:46:16.054984",
     "exception": false,
     "start_time": "2023-12-12T01:46:16.032146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EyeImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.dataset = datasets.ImageFolder(root=root_dir, transform=transform)\n",
    "        self.classes = self.dataset.classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e9ee6f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T01:46:16.083149Z",
     "iopub.status.busy": "2023-12-12T01:46:16.082838Z",
     "iopub.status.idle": "2023-12-12T01:46:18.024210Z",
     "shell.execute_reply": "2023-12-12T01:46:18.023240Z"
    },
    "papermill": {
     "duration": 1.957689,
     "end_time": "2023-12-12T01:46:18.026486",
     "exception": false,
     "start_time": "2023-12-12T01:46:16.068797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_directory = '/kaggle/input/dataset/'\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size = (224, 224)),\n",
    "    transforms.RandomRotation(degrees=5),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "])\n",
    "\n",
    "dataset = EyeImageDataset(root_dir=root_directory, transform=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "853845e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T01:46:18.054452Z",
     "iopub.status.busy": "2023-12-12T01:46:18.054148Z",
     "iopub.status.idle": "2023-12-12T01:46:18.074424Z",
     "shell.execute_reply": "2023-12-12T01:46:18.073591Z"
    },
    "papermill": {
     "duration": 0.036263,
     "end_time": "2023-12-12T01:46:18.076357",
     "exception": false,
     "start_time": "2023-12-12T01:46:18.040094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [int(len(dataset)*0.9), len(dataset)-int(len(dataset)*0.9)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a046d89e",
   "metadata": {
    "papermill": {
     "duration": 0.01376,
     "end_time": "2023-12-12T01:46:18.103731",
     "exception": false,
     "start_time": "2023-12-12T01:46:18.089971",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setting number of clients to simulate Federated Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d6bf000",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T01:46:18.132260Z",
     "iopub.status.busy": "2023-12-12T01:46:18.131523Z",
     "iopub.status.idle": "2023-12-12T01:46:18.135480Z",
     "shell.execute_reply": "2023-12-12T01:46:18.134675Z"
    },
    "papermill": {
     "duration": 0.020012,
     "end_time": "2023-12-12T01:46:18.137333",
     "exception": false,
     "start_time": "2023-12-12T01:46:18.117321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_CLIENTS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65a5900f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T01:46:18.164384Z",
     "iopub.status.busy": "2023-12-12T01:46:18.164141Z",
     "iopub.status.idle": "2023-12-12T01:46:18.172304Z",
     "shell.execute_reply": "2023-12-12T01:46:18.171487Z"
    },
    "papermill": {
     "duration": 0.024039,
     "end_time": "2023-12-12T01:46:18.174216",
     "exception": false,
     "start_time": "2023-12-12T01:46:18.150177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_clients = NUM_CLIENTS\n",
    "\n",
    "# Split training set into `num_clients` partitions to simulate different local datasets\n",
    "partition_size = len(train_dataset) // num_clients\n",
    "lengths = [partition_size] * num_clients\n",
    "datasets = random_split(train_dataset, lengths, torch.Generator().manual_seed(42))\n",
    "\n",
    "# Split each partition into train/val and create DataLoader\n",
    "train_loaders = []\n",
    "val_loaders = []\n",
    "for ds in datasets:\n",
    "    len_val = len(ds) // 10  # 10 % validation set\n",
    "    len_train = len(ds) - len_val\n",
    "    lengths = [len_train, len_val]\n",
    "    ds_train, ds_val = random_split(ds, lengths, torch.Generator().manual_seed(42))\n",
    "    train_loaders.append(DataLoader(ds_train, batch_size=8, shuffle=True))\n",
    "    val_loaders.append(DataLoader(ds_val, batch_size=8,shuffle=True))\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba27e25",
   "metadata": {
    "id": "OBp7kB4G0sPB",
    "papermill": {
     "duration": 0.01298,
     "end_time": "2023-12-12T01:46:18.201103",
     "exception": false,
     "start_time": "2023-12-12T01:46:18.188123",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Model training/evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "928b9bf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T01:46:18.229490Z",
     "iopub.status.busy": "2023-12-12T01:46:18.229179Z",
     "iopub.status.idle": "2023-12-12T01:46:18.233611Z",
     "shell.execute_reply": "2023-12-12T01:46:18.232824Z"
    },
    "papermill": {
     "duration": 0.020837,
     "end_time": "2023-12-12T01:46:18.235481",
     "exception": false,
     "start_time": "2023-12-12T01:46:18.214644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision.models import vgg19, VGG19_Weights\n",
    "import torch.nn as nn\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b384191",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T01:46:18.262690Z",
     "iopub.status.busy": "2023-12-12T01:46:18.262409Z",
     "iopub.status.idle": "2023-12-12T01:46:22.595407Z",
     "shell.execute_reply": "2023-12-12T01:46:22.594147Z"
    },
    "papermill": {
     "duration": 4.349409,
     "end_time": "2023-12-12T01:46:22.598006",
     "exception": false,
     "start_time": "2023-12-12T01:46:18.248597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
      "100%|██████████| 548M/548M [00:02<00:00, 223MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = vgg19(weights = VGG19_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# freeze all the layers of VGG\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b954e9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T01:46:22.632150Z",
     "iopub.status.busy": "2023-12-12T01:46:22.631374Z",
     "iopub.status.idle": "2023-12-12T01:46:22.672692Z",
     "shell.execute_reply": "2023-12-12T01:46:22.671947Z"
    },
    "papermill": {
     "duration": 0.060445,
     "end_time": "2023-12-12T01:46:22.674835",
     "exception": false,
     "start_time": "2023-12-12T01:46:22.614390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# modify the classifier layer\n",
    "model.classifier = nn.Sequential(*list(model.classifier.children())[:-1], nn.Linear(4096, 1024), nn.ReLU(), nn.Linear(1024,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38c61aea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T01:46:22.707874Z",
     "iopub.status.busy": "2023-12-12T01:46:22.707198Z",
     "iopub.status.idle": "2023-12-12T01:46:22.712006Z",
     "shell.execute_reply": "2023-12-12T01:46:22.711166Z"
    },
    "papermill": {
     "duration": 0.023511,
     "end_time": "2023-12-12T01:46:22.713921",
     "exception": false,
     "start_time": "2023-12-12T01:46:22.690410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ct = 0\n",
    "for child in model.features:\n",
    "    ct += 1\n",
    "    if ct > 27:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22a2682e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T01:46:22.793993Z",
     "iopub.status.busy": "2023-12-12T01:46:22.793244Z",
     "iopub.status.idle": "2023-12-12T01:46:22.797981Z",
     "shell.execute_reply": "2023-12-12T01:46:22.797054Z"
    },
    "papermill": {
     "duration": 0.023822,
     "end_time": "2023-12-12T01:46:22.799947",
     "exception": false,
     "start_time": "2023-12-12T01:46:22.776125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# allow learning only for the linear layer\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "423a037e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T01:46:22.833217Z",
     "iopub.status.busy": "2023-12-12T01:46:22.832928Z",
     "iopub.status.idle": "2023-12-12T01:46:25.730283Z",
     "shell.execute_reply": "2023-12-12T01:46:25.728692Z"
    },
    "papermill": {
     "duration": 2.916857,
     "end_time": "2023-12-12T01:46:25.732872",
     "exception": false,
     "start_time": "2023-12-12T01:46:22.816015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (24): ReLU(inplace=True)\n",
      "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): ReLU(inplace=True)\n",
      "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (33): ReLU(inplace=True)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): ReLU(inplace=True)\n",
      "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=1024, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = model.to(DEVICE)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d37c279",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T01:46:25.778293Z",
     "iopub.status.busy": "2023-12-12T01:46:25.777671Z",
     "iopub.status.idle": "2023-12-12T01:46:25.793960Z",
     "shell.execute_reply": "2023-12-12T01:46:25.793150Z"
    },
    "papermill": {
     "duration": 0.039691,
     "end_time": "2023-12-12T01:46:25.795984",
     "exception": false,
     "start_time": "2023-12-12T01:46:25.756293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, criterion):\n",
    "    \"\"\"Evaluate the model on the given dataset.\"\"\"\n",
    "    # Set the model to evaluation mode.\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    val_loss = 0\n",
    "    count = 0\n",
    "    # The `torch.no_grad()` context will turn off gradients for efficiency.\n",
    "    with torch.no_grad():\n",
    "        for images, labels in (data_loader):\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            output = model(images)\n",
    "            pred = output.argmax(dim=1)\n",
    "            loss = criterion(output, labels)\n",
    "            correct += (pred == labels).sum().item()\n",
    "            val_loss += loss.item() \n",
    "            count += 1\n",
    "    return correct / len(data_loader.dataset), val_loss/count\n",
    "\n",
    "\n",
    "def train(model, n_epoch, optimizer, scheduler, criterion, train_loader, valid_loader):\n",
    "    \"\"\"Train the model on the given dataset.\"\"\"\n",
    "    loss_ref = float('inf')\n",
    "    for epoch in range(n_epoch):\n",
    "        # Set the model to training mode.\n",
    "        model.train()\n",
    "        for step, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            # 1. Clear previous gradients.\n",
    "            optimizer.zero_grad()\n",
    "            # 2. Forward pass. Calculate the output of the model.\n",
    "            output = model(images)\n",
    "            # 3. Calculate the loss.\n",
    "            loss = criterion(output, labels)\n",
    "            # 4. Calculate the gradients. PyTorch does this for us!\n",
    "            loss.backward()\n",
    "            # 5. Update the model parameters.\n",
    "            optimizer.step()\n",
    "            if step % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Step {step}, Loss {loss.item():.4f}\")\n",
    "        \n",
    "        acc, val_loss = evaluate(model, valid_loader, criterion)\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch}, Valid Accuracy {acc * 100:.2f}%\") \n",
    "        \n",
    "        if val_loss < loss_ref:\n",
    "            patience = 5\n",
    "            loss_ref = val_loss\n",
    "        else:\n",
    "            if patience == 0:\n",
    "                print(f\"[Early Stopping] Epoch {epoch}, Valid Accuracy {acc * 100:.2f}%, Valid Loss {val_loss:.4f}\")\n",
    "                return\n",
    "            print(f\"[INFO] Patience {patience} remaining\")\n",
    "            patience-=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a83de96e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T01:46:25.828633Z",
     "iopub.status.busy": "2023-12-12T01:46:25.827909Z",
     "iopub.status.idle": "2023-12-12T01:46:25.840620Z",
     "shell.execute_reply": "2023-12-12T01:46:25.839741Z"
    },
    "id": "2X3cVBXMpP6w",
    "papermill": {
     "duration": 0.03111,
     "end_time": "2023-12-12T01:46:25.842754",
     "exception": false,
     "start_time": "2023-12-12T01:46:25.811644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_parameters(net) -> List[np.ndarray]:\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "\n",
    "\n",
    "def set_parameters(net, parameters: List[np.ndarray]):\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict(\n",
    "            {\n",
    "                k: torch.Tensor(v) if v.shape != torch.Size([]) else torch.Tensor([0])\n",
    "                for k, v in params_dict\n",
    "            }\n",
    "        )\n",
    "    net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "def test(net, testloader):\n",
    "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    loss /= len(testloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5020fdb",
   "metadata": {
    "id": "1lCf3oljdClM",
    "papermill": {
     "duration": 0.015086,
     "end_time": "2023-12-12T01:46:25.873145",
     "exception": false,
     "start_time": "2023-12-12T01:46:25.858059",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Flower client\n",
    "\n",
    "Setting flower client. These flower clients represent a client on the federated learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "836816c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T01:46:25.905143Z",
     "iopub.status.busy": "2023-12-12T01:46:25.904425Z",
     "iopub.status.idle": "2023-12-12T01:46:25.921534Z",
     "shell.execute_reply": "2023-12-12T01:46:25.920738Z"
    },
    "id": "ye6Jt5p3LWtF",
    "papermill": {
     "duration": 0.035658,
     "end_time": "2023-12-12T01:46:25.923763",
     "exception": false,
     "start_time": "2023-12-12T01:46:25.888105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from logging import INFO, DEBUG\n",
    "from flwr.common.logger import log\n",
    "\n",
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, cid, net, trainloader, valloader):\n",
    "        self.cid = cid\n",
    "        self.model = model.to(DEVICE)\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "\n",
    "    def get_parameters(self):\n",
    "        print(f\"[Client {self.cid}] get_parameters\")\n",
    "        return [val.cpu().numpy() for _, val in self.model.state_dict().items()]\n",
    "    \n",
    "    def set_parameters(self, parameters: List[np.ndarray]) -> None:\n",
    "        # Set model parameters from a list of NumPy ndarrays\n",
    "        params_dict = zip(self.model.state_dict().keys(), parameters)\n",
    "        state_dict = OrderedDict(\n",
    "                    {\n",
    "                        k: torch.Tensor(v) if v.shape != torch.Size([]) else torch.Tensor([0])\n",
    "                        for k, v in params_dict\n",
    "                    }\n",
    "                )\n",
    "        self.model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        print(f\"[Client {self.cid}] fit, config: {config}\")\n",
    "        self.set_parameters(parameters)\n",
    "        lr = 1e-3\n",
    "        gamma = 0.9\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, verbose=True)\n",
    "\n",
    "        n_epoch = 20\n",
    "        train(self.model, n_epoch, optimizer, scheduler,criterion,self.trainloader, self.valloader)\n",
    "        return self.get_parameters(), len(self.trainloader), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        print(f\"[Client {self.cid}] evaluate, config: {config}\")\n",
    "        self.set_parameters(parameters)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        accuracy, loss = evaluate(self.model, self.valloader, criterion)\n",
    "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}\n",
    "\n",
    "\n",
    "def client_fn(cid) -> FlowerClient:\n",
    "    train_loader = train_loaders[int(cid)]\n",
    "    val_loader = val_loaders[int(cid)]\n",
    "    return FlowerClient(cid, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505aaa04",
   "metadata": {
    "papermill": {
     "duration": 0.015233,
     "end_time": "2023-12-12T01:46:25.958023",
     "exception": false,
     "start_time": "2023-12-12T01:46:25.942790",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Server Side Evaluation\n",
    "We evalaute the accumulated model on the server after every federated round against the validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2cc5404",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T01:46:25.990632Z",
     "iopub.status.busy": "2023-12-12T01:46:25.989713Z",
     "iopub.status.idle": "2023-12-12T01:46:25.998182Z",
     "shell.execute_reply": "2023-12-12T01:46:25.997365Z"
    },
    "id": "MDovnUvsn7if",
    "papermill": {
     "duration": 0.02673,
     "end_time": "2023-12-12T01:46:26.000054",
     "exception": false,
     "start_time": "2023-12-12T01:46:25.973324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_evaluate_fn():\n",
    "\n",
    "    def evaluate(\n",
    "        parameters:  List[np.ndarray],\n",
    "    ) -> Optional[Tuple[float, Dict[str, fl.common.Scalar]]]:\n",
    "        params_dict = zip(model.state_dict().keys(), parameters)\n",
    "        state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
    "        model.load_state_dict(state_dict, strict=True)\n",
    "        val_loss, val_accuracy = test(model, val_loaders[0])\n",
    "        test_loss, test_accuracy = test(model, test_loader)\n",
    "        return val_loss, {\"val_accuracy\": val_accuracy, \"test_accuracy\": test_accuracy}\n",
    "\n",
    "    return evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70af53f4",
   "metadata": {
    "papermill": {
     "duration": 0.016165,
     "end_time": "2023-12-12T01:46:26.033259",
     "exception": false,
     "start_time": "2023-12-12T01:46:26.017094",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Server Side Initialization and Simulation\n",
    "We make use of FedAvg to accumulate the paramters from all the clients on the server. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "232976df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T01:46:26.069348Z",
     "iopub.status.busy": "2023-12-12T01:46:26.068325Z",
     "iopub.status.idle": "2023-12-12T02:55:27.677754Z",
     "shell.execute_reply": "2023-12-12T02:55:27.676702Z"
    },
    "id": "r5aGRnyQnu0Z",
    "papermill": {
     "duration": 4141.696414,
     "end_time": "2023-12-12T02:55:27.745997",
     "exception": false,
     "start_time": "2023-12-12T01:46:26.049583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flower 2023-12-12 01:46:31,489 | app.py:144 | Ray initialized with resources: {'accelerator_type:T4': 1.0, 'memory': 18233032704.0, 'node:172.19.2.2': 1.0, 'object_store_memory': 9116516352.0, 'GPU': 2.0, 'CPU': 4.0}\n",
      "INFO flower 2023-12-12 01:46:31,493 | app.py:153 | Starting Flower simulation running: {'num_rounds': 7}\n",
      "INFO flower 2023-12-12 01:46:31,494 | server.py:128 | Initializing global parameters\n",
      "INFO flower 2023-12-12 01:46:31,495 | server.py:323 | Using initial parameters provided by strategy\n",
      "INFO flower 2023-12-12 01:46:31,496 | server.py:130 | Evaluating initial parameters\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "INFO flower 2023-12-12 01:46:56,315 | server.py:133 | initial parameters (loss, other metrics): 0.1779284534000215, {'val_accuracy': 0.25396825396825395, 'test_accuracy': 0.27014218009478674}\n",
      "INFO flower 2023-12-12 01:46:56,316 | server.py:143 | FL starting\n",
      "DEBUG flower 2023-12-12 01:46:56,317 | server.py:265 | fit_round: strategy sampled 2 clients (out of 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m [Client 1] fit, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m /opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 0, Step 0, Loss 1.3318\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 0, Step 10, Loss 1.8281\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 0, Step 20, Loss 1.0988\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 0, Step 30, Loss 1.1148\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 0, Step 40, Loss 1.2578\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 0, Step 50, Loss 1.8998\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 0, Step 60, Loss 1.3898\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 0, Step 70, Loss 1.3892\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 0, Step 80, Loss 1.3596\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 0, Step 90, Loss 1.3920\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 0, Step 100, Loss 1.4071\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 0, Step 110, Loss 1.4048\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 0, Step 120, Loss 1.3850\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 0, Step 130, Loss 1.3847\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 0, Step 140, Loss 1.3783\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 0, Valid Accuracy 26.98%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 1, Step 0, Loss 1.3855\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 1, Step 10, Loss 1.3824\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 1, Step 20, Loss 1.3915\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 1, Step 30, Loss 1.3911\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 1, Step 40, Loss 1.3895\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 1, Step 50, Loss 1.3907\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 1, Step 60, Loss 1.3893\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 1, Step 70, Loss 1.3872\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 1, Step 80, Loss 1.3916\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 1, Step 90, Loss 1.4033\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 1, Step 100, Loss 1.3967\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 1, Step 110, Loss 1.3904\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 1, Step 120, Loss 1.3912\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 1, Step 130, Loss 1.3871\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 1, Step 140, Loss 1.3972\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 1, Valid Accuracy 26.98%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m [INFO] Patience 5 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 2, Step 0, Loss 1.4133\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 2, Step 10, Loss 1.3642\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 2, Step 20, Loss 1.3990\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 2, Step 30, Loss 1.3962\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 2, Step 40, Loss 1.3825\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 2, Step 50, Loss 1.3852\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 2, Step 60, Loss 1.3922\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 2, Step 70, Loss 1.3951\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 2, Step 80, Loss 1.3820\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 2, Step 90, Loss 1.4011\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 2, Step 100, Loss 1.3456\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 2, Step 110, Loss 1.4187\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 2, Step 120, Loss 1.4433\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 2, Step 130, Loss 1.3776\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 2, Step 140, Loss 1.4123\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 2, Valid Accuracy 26.98%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m [INFO] Patience 4 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 3, Step 0, Loss 1.4152\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 3, Step 10, Loss 1.3906\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 3, Step 20, Loss 1.3954\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 3, Step 30, Loss 1.4341\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 3, Step 40, Loss 1.4083\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 3, Step 50, Loss 1.4208\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 3, Step 60, Loss 1.3781\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 3, Step 70, Loss 1.3791\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 3, Step 80, Loss 1.3885\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 3, Step 90, Loss 1.4079\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 3, Step 100, Loss 1.4212\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 3, Step 110, Loss 1.3846\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 3, Step 120, Loss 1.3490\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 3, Step 130, Loss 1.3797\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 3, Step 140, Loss 1.3733\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 3, Valid Accuracy 17.46%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m [INFO] Patience 3 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 4, Step 0, Loss 1.3914\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 4, Step 10, Loss 1.3889\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 4, Step 20, Loss 1.3923\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 4, Step 30, Loss 1.4046\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 4, Step 40, Loss 1.4063\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 4, Step 50, Loss 1.3726\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 4, Step 60, Loss 1.3851\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 4, Step 70, Loss 1.4027\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 4, Step 80, Loss 1.3622\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 4, Step 90, Loss 1.4096\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 4, Step 100, Loss 1.3750\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 4, Step 110, Loss 1.3724\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 4, Step 120, Loss 1.3815\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 4, Step 130, Loss 1.3714\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 4, Step 140, Loss 1.4057\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 00005: reducing learning rate of group 0 to 1.0000e-04.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 4, Valid Accuracy 17.46%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m [INFO] Patience 2 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 5, Step 0, Loss 1.3565\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 5, Step 10, Loss 1.4058\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 5, Step 20, Loss 1.3945\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 5, Step 30, Loss 1.4212\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 5, Step 40, Loss 1.4148\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 5, Step 50, Loss 1.3875\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 5, Step 60, Loss 1.3793\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 5, Step 70, Loss 1.3781\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 5, Step 80, Loss 1.3813\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 5, Step 90, Loss 1.3616\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 5, Step 100, Loss 1.4079\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 5, Step 110, Loss 1.3662\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 5, Step 120, Loss 1.3843\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 5, Step 130, Loss 1.4202\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 5, Step 140, Loss 1.3995\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 5, Valid Accuracy 17.46%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m [INFO] Patience 1 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 6, Step 0, Loss 1.3420\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 6, Step 10, Loss 1.3587\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 6, Step 20, Loss 1.3908\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 6, Step 30, Loss 1.3715\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 6, Step 40, Loss 1.3889\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 6, Step 50, Loss 1.3967\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 6, Step 60, Loss 1.3909\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 6, Step 70, Loss 1.3652\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 6, Step 80, Loss 1.3870\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 6, Step 90, Loss 1.3722\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 6, Step 100, Loss 1.3743\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 6, Step 110, Loss 1.3834\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 6, Step 120, Loss 1.3987\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 6, Step 130, Loss 1.3779\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 6, Step 140, Loss 1.4120\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m Epoch 6, Valid Accuracy 17.46%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m [Early Stopping] Epoch 6, Valid Accuracy 17.46%, Valid Loss 1.3941\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=305)\u001b[0m [Client 1] get_parameters\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m [Client 2] fit, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m /opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 0, Step 0, Loss 1.4370\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 0, Step 10, Loss 1.4464\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 0, Step 20, Loss 1.5677\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 0, Step 30, Loss 2.7974\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 0, Step 40, Loss 1.4046\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 0, Step 50, Loss 1.3869\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 0, Step 60, Loss 1.3810\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 0, Step 70, Loss 1.3655\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 0, Step 80, Loss 1.4307\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 0, Step 90, Loss 1.4184\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 0, Step 100, Loss 1.3917\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 0, Step 110, Loss 1.3883\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 0, Step 120, Loss 1.3764\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 0, Step 130, Loss 1.3882\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 0, Step 140, Loss 1.4031\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 0, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 1, Step 0, Loss 1.4121\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 1, Step 10, Loss 1.3792\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 1, Step 20, Loss 1.3771\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 1, Step 30, Loss 1.3830\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 1, Step 40, Loss 1.4488\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 1, Step 50, Loss 1.3676\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 1, Step 60, Loss 1.3851\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 1, Step 70, Loss 1.3456\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 1, Step 80, Loss 1.0180\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 1, Step 90, Loss 1.3993\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 1, Step 100, Loss 1.4233\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 1, Step 110, Loss 1.4027\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 1, Step 120, Loss 1.3686\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 1, Step 130, Loss 1.3830\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 1, Step 140, Loss 1.4242\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 1, Valid Accuracy 26.19%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m [INFO] Patience 5 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 2, Step 0, Loss 1.4433\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 2, Step 10, Loss 1.4606\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 2, Step 20, Loss 1.3951\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 2, Step 30, Loss 1.3902\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 2, Step 40, Loss 1.4422\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 2, Step 50, Loss 1.4182\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 2, Step 60, Loss 1.3895\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 2, Step 70, Loss 1.3709\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 2, Step 80, Loss 1.3697\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 2, Step 90, Loss 1.3666\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 2, Step 100, Loss 1.3572\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 2, Step 110, Loss 1.3420\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 2, Step 120, Loss 1.4713\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 2, Step 130, Loss 1.4619\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 2, Step 140, Loss 1.4112\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 2, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m [INFO] Patience 4 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 3, Step 0, Loss 1.3416\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 3, Step 10, Loss 1.4017\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 3, Step 20, Loss 1.3619\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 3, Step 30, Loss 1.5071\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 3, Step 40, Loss 1.4221\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 3, Step 50, Loss 1.3801\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 3, Step 60, Loss 1.3547\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 3, Step 70, Loss 1.3803\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 3, Step 80, Loss 1.4606\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 3, Step 90, Loss 1.4015\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 3, Step 100, Loss 1.3942\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 3, Step 110, Loss 1.3893\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 3, Step 120, Loss 1.3836\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 3, Step 130, Loss 1.3750\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 3, Step 140, Loss 1.4096\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 3, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 4, Step 0, Loss 1.3869\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 4, Step 10, Loss 1.3949\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 4, Step 20, Loss 1.3517\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 4, Step 30, Loss 1.4003\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 4, Step 40, Loss 1.3898\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 4, Step 50, Loss 1.3915\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 4, Step 60, Loss 1.3811\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 4, Step 70, Loss 1.3504\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 4, Step 80, Loss 1.3834\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 4, Step 90, Loss 1.3639\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 4, Step 100, Loss 1.3848\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 4, Step 110, Loss 1.3710\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 4, Step 120, Loss 1.3782\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 4, Step 130, Loss 1.3434\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 4, Step 140, Loss 1.4007\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 4, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m [INFO] Patience 5 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 5, Step 0, Loss 1.4116\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 5, Step 10, Loss 1.3393\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 5, Step 20, Loss 1.4621\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 5, Step 30, Loss 1.4019\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 5, Step 40, Loss 1.3773\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 5, Step 50, Loss 1.3040\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 5, Step 60, Loss 1.4505\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 5, Step 70, Loss 1.3781\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 5, Step 80, Loss 1.4101\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 5, Step 90, Loss 1.3718\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 5, Step 100, Loss 1.3635\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 5, Step 110, Loss 1.3943\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 5, Step 120, Loss 1.4081\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 5, Step 130, Loss 1.4428\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 5, Step 140, Loss 1.4012\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 5, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m [INFO] Patience 4 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 6, Step 0, Loss 1.3762\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 6, Step 10, Loss 1.3794\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 6, Step 20, Loss 1.3873\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 6, Step 30, Loss 1.3937\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 6, Step 40, Loss 1.3773\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 6, Step 50, Loss 1.3395\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 6, Step 60, Loss 1.3666\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 6, Step 70, Loss 1.3713\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 6, Step 80, Loss 1.4019\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 6, Step 90, Loss 1.3356\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 6, Step 100, Loss 1.3311\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 6, Step 110, Loss 1.3434\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 6, Step 120, Loss 1.3331\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 6, Step 130, Loss 1.3945\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 6, Step 140, Loss 1.3908\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 6, Valid Accuracy 26.19%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m [INFO] Patience 3 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 7, Step 0, Loss 1.3646\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 7, Step 10, Loss 1.4387\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 7, Step 20, Loss 1.3931\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 7, Step 30, Loss 1.3840\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 7, Step 40, Loss 1.3581\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 7, Step 50, Loss 1.2470\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 7, Step 60, Loss 1.3487\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 7, Step 70, Loss 1.3349\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 7, Step 80, Loss 1.4409\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 7, Step 90, Loss 1.4131\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 7, Step 100, Loss 1.3728\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 7, Step 110, Loss 1.3959\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 7, Step 120, Loss 1.3980\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 7, Step 130, Loss 1.3926\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 7, Step 140, Loss 1.4082\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 00008: reducing learning rate of group 0 to 1.0000e-04.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 7, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m [INFO] Patience 2 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 8, Step 0, Loss 1.3785\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 8, Step 10, Loss 1.3948\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 8, Step 20, Loss 1.3738\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 8, Step 30, Loss 1.3794\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 8, Step 40, Loss 1.3824\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 8, Step 50, Loss 1.3815\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 8, Step 60, Loss 1.3648\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 8, Step 70, Loss 1.3808\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 8, Step 80, Loss 1.3952\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 8, Step 90, Loss 1.3969\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 8, Step 100, Loss 1.3723\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 8, Step 110, Loss 1.3753\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 8, Step 120, Loss 1.3978\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 8, Step 130, Loss 1.3567\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 8, Step 140, Loss 1.3871\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 8, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m [INFO] Patience 1 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 9, Step 0, Loss 1.4090\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 9, Step 10, Loss 1.4208\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 9, Step 20, Loss 1.3652\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 9, Step 30, Loss 1.3865\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 9, Step 40, Loss 1.3680\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 9, Step 50, Loss 1.4078\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 9, Step 60, Loss 1.3529\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 9, Step 70, Loss 1.3741\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 9, Step 80, Loss 1.3772\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 9, Step 90, Loss 1.3879\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 9, Step 100, Loss 1.3974\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 9, Step 110, Loss 1.3712\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 9, Step 120, Loss 1.4177\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 9, Step 130, Loss 1.3384\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 9, Step 140, Loss 1.3854\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m Epoch 9, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m [Early Stopping] Epoch 9, Valid Accuracy 19.84%, Valid Loss 1.3938\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=498)\u001b[0m [Client 2] get_parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flower 2023-12-12 01:56:31,292 | server.py:277 | fit_round received 2 results and 0 failures\n",
      "INFO flower 2023-12-12 01:56:46,322 | server.py:158 | fit progress: (1, 0.17626559355902294, {'val_accuracy': 0.25396825396825395, 'test_accuracy': 0.26540284360189575}, 590.005546631)\n",
      "INFO flower 2023-12-12 01:56:46,323 | server.py:209 | evaluate_round: no clients selected, cancel\n",
      "DEBUG flower 2023-12-12 01:56:46,325 | server.py:265 | fit_round: strategy sampled 2 clients (out of 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m [Client 1] fit, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m /opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 0, Step 0, Loss 1.3823\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 0, Step 10, Loss 1.2906\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 0, Step 20, Loss 1.4440\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 0, Step 30, Loss 1.3386\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 0, Step 40, Loss 1.3821\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 0, Step 50, Loss 1.3695\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 0, Step 60, Loss 1.3623\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 0, Step 70, Loss 1.3805\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 0, Step 80, Loss 1.3655\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 0, Step 90, Loss 1.3908\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 0, Step 100, Loss 1.3982\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 0, Step 110, Loss 1.4016\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 0, Step 120, Loss 1.3913\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 0, Step 130, Loss 1.3756\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 0, Step 140, Loss 1.3927\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 0, Valid Accuracy 17.46%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 1, Step 0, Loss 1.3902\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 1, Step 10, Loss 1.4069\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 1, Step 20, Loss 1.3801\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 1, Step 30, Loss 1.3950\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 1, Step 40, Loss 1.3928\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 1, Step 50, Loss 1.4058\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 1, Step 60, Loss 1.3977\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 1, Step 70, Loss 1.3899\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 1, Step 80, Loss 1.3915\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 1, Step 90, Loss 1.3801\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 1, Step 100, Loss 1.3826\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 1, Step 110, Loss 1.4192\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 1, Step 120, Loss 1.3749\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 1, Step 130, Loss 1.3844\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 1, Step 140, Loss 1.4364\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 1, Valid Accuracy 17.46%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m [INFO] Patience 5 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 2, Step 0, Loss 1.4007\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 2, Step 10, Loss 1.4038\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 2, Step 20, Loss 1.4059\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 2, Step 30, Loss 1.4090\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 2, Step 40, Loss 1.3884\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 2, Step 50, Loss 1.3796\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 2, Step 60, Loss 1.3808\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 2, Step 70, Loss 1.3762\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 2, Step 80, Loss 1.3785\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 2, Step 90, Loss 1.3950\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 2, Step 100, Loss 1.3484\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 2, Step 110, Loss 1.3858\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 2, Step 120, Loss 1.4077\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 2, Step 130, Loss 1.3879\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 2, Step 140, Loss 1.3644\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 2, Valid Accuracy 26.98%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m [INFO] Patience 4 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 3, Step 0, Loss 1.3792\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 3, Step 10, Loss 1.3960\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 3, Step 20, Loss 1.3949\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 3, Step 30, Loss 1.3694\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 3, Step 40, Loss 1.3805\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 3, Step 50, Loss 1.3769\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 3, Step 60, Loss 1.4211\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 3, Step 70, Loss 1.3635\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 3, Step 80, Loss 1.3617\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 3, Step 90, Loss 1.4052\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 3, Step 100, Loss 1.3684\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 3, Step 110, Loss 1.3898\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 3, Step 120, Loss 1.4468\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 3, Step 130, Loss 1.3460\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 3, Step 140, Loss 1.4053\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 3, Valid Accuracy 17.46%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m [INFO] Patience 3 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 4, Step 0, Loss 1.3870\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 4, Step 10, Loss 1.3484\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 4, Step 20, Loss 1.4395\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 4, Step 30, Loss 1.3789\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 4, Step 40, Loss 1.3813\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 4, Step 50, Loss 1.3762\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 4, Step 60, Loss 1.3556\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 4, Step 70, Loss 1.3788\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 4, Step 80, Loss 1.4206\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 4, Step 90, Loss 1.3967\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 4, Step 100, Loss 1.3850\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 4, Step 110, Loss 1.3994\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 4, Step 120, Loss 1.3895\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 4, Step 130, Loss 1.3970\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 4, Step 140, Loss 1.4078\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 00005: reducing learning rate of group 0 to 1.0000e-04.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 4, Valid Accuracy 17.46%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m [INFO] Patience 2 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 5, Step 0, Loss 1.3570\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 5, Step 10, Loss 1.4364\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 5, Step 20, Loss 1.4015\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 5, Step 30, Loss 1.4013\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 5, Step 40, Loss 1.3703\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 5, Step 50, Loss 1.3700\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 5, Step 60, Loss 1.3756\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 5, Step 70, Loss 1.4088\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 5, Step 80, Loss 1.3711\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 5, Step 90, Loss 1.3917\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 5, Step 100, Loss 1.3962\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 5, Step 110, Loss 1.3788\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 5, Step 120, Loss 1.3720\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 5, Step 130, Loss 1.3837\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 5, Step 140, Loss 1.3765\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 5, Valid Accuracy 17.46%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m [INFO] Patience 1 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 6, Step 0, Loss 1.3844\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 6, Step 10, Loss 1.4003\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 6, Step 20, Loss 1.3989\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 6, Step 30, Loss 1.4178\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 6, Step 40, Loss 1.3942\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 6, Step 50, Loss 1.3703\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 6, Step 60, Loss 1.3799\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 6, Step 70, Loss 1.4021\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 6, Step 80, Loss 1.4051\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 6, Step 90, Loss 1.3639\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 6, Step 100, Loss 1.3887\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 6, Step 110, Loss 1.3729\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 6, Step 120, Loss 1.3872\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 6, Step 130, Loss 1.4039\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 6, Step 140, Loss 1.3747\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m Epoch 6, Valid Accuracy 17.46%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m [Early Stopping] Epoch 6, Valid Accuracy 17.46%, Valid Loss 1.3917\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=768)\u001b[0m [Client 1] get_parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=960)\u001b[0m 2023-12-12 02:00:48,588\tERROR serialization.py:342 -- CUDA error: out of memory\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=960)\u001b[0m CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=960)\u001b[0m For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=960)\u001b[0m Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=960)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=960)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/ray/serialization.py\", line 340, in deserialize_objects\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=960)\u001b[0m     obj = self._deserialize_object(data, metadata, object_ref)\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=960)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/ray/serialization.py\", line 237, in _deserialize_object\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=960)\u001b[0m     return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=960)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/ray/serialization.py\", line 192, in _deserialize_msgpack_data\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=960)\u001b[0m     python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=960)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/ray/serialization.py\", line 182, in _deserialize_pickle5_data\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=960)\u001b[0m     obj = pickle.loads(in_band)\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=960)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=960)\u001b[0m     return torch.load(io.BytesIO(b))\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=960)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=960)\u001b[0m     return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=960)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=960)\u001b[0m     result = unpickler.load()\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=960)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=960)\u001b[0m     wrap_storage=restore_location(obj, location),\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=960)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=960)\u001b[0m     result = fn(storage, location)\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=960)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=960)\u001b[0m     return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=960)\u001b[0m RuntimeError: CUDA error: out of memory\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=960)\u001b[0m CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=960)\u001b[0m For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=960)\u001b[0m Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=960)\u001b[0m \n",
      "DEBUG flower 2023-12-12 02:00:49,070 | server.py:277 | fit_round received 1 results and 1 failures\n",
      "INFO flower 2023-12-12 02:01:05,708 | server.py:158 | fit progress: (2, 0.17638013192585536, {'val_accuracy': 0.25396825396825395, 'test_accuracy': 0.26540284360189575}, 849.391385745)\n",
      "INFO flower 2023-12-12 02:01:05,709 | server.py:209 | evaluate_round: no clients selected, cancel\n",
      "DEBUG flower 2023-12-12 02:01:05,710 | server.py:265 | fit_round: strategy sampled 2 clients (out of 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m [Client 0] fit, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m /opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 0, Step 0, Loss 1.3856\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 0, Step 10, Loss 1.3805\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 0, Step 20, Loss 1.4148\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 0, Step 30, Loss 1.3645\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 0, Step 40, Loss 1.4000\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 0, Step 50, Loss 1.4740\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 0, Step 60, Loss 1.3834\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 0, Step 70, Loss 1.3721\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 0, Step 80, Loss 1.3756\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 0, Step 90, Loss 1.3848\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 0, Step 100, Loss 1.4128\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 0, Step 110, Loss 1.3688\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 0, Step 120, Loss 1.3840\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 0, Step 130, Loss 1.3967\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 0, Step 140, Loss 1.3568\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 0, Valid Accuracy 26.19%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 1, Step 0, Loss 1.3785\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 1, Step 10, Loss 1.4091\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 1, Step 20, Loss 1.3019\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 1, Step 30, Loss 1.3574\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 1, Step 40, Loss 1.3807\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 1, Step 50, Loss 1.3613\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 1, Step 60, Loss 1.3528\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 1, Step 70, Loss 1.3930\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 1, Step 80, Loss 1.3882\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 1, Step 90, Loss 1.3925\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 1, Step 100, Loss 1.3784\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 1, Step 110, Loss 1.3631\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 1, Step 120, Loss 1.3974\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 1, Step 130, Loss 1.3440\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 1, Step 140, Loss 1.3817\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 1, Valid Accuracy 26.19%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 2, Step 0, Loss 1.4049\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 2, Step 10, Loss 1.3803\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 2, Step 20, Loss 1.3803\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 2, Step 30, Loss 1.3741\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 2, Step 40, Loss 1.3805\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 2, Step 50, Loss 1.3886\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 2, Step 60, Loss 1.3807\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 2, Step 70, Loss 1.3807\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 2, Step 80, Loss 1.3675\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 2, Step 90, Loss 1.4236\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 2, Step 100, Loss 1.4131\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 2, Step 110, Loss 1.3689\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 2, Step 120, Loss 1.4558\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 2, Step 130, Loss 1.3604\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 2, Step 140, Loss 1.4222\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 2, Valid Accuracy 26.19%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m [INFO] Patience 5 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 3, Step 0, Loss 1.4317\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 3, Step 10, Loss 1.3334\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 3, Step 20, Loss 1.3732\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 3, Step 30, Loss 1.4340\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 3, Step 40, Loss 1.3606\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 3, Step 50, Loss 1.3779\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 3, Step 60, Loss 1.3733\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 3, Step 70, Loss 1.3358\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 3, Step 80, Loss 1.4008\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 3, Step 90, Loss 1.4220\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 3, Step 100, Loss 1.3660\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 3, Step 110, Loss 1.3065\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 3, Step 120, Loss 1.4806\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 3, Step 130, Loss 1.4168\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 3, Step 140, Loss 1.4049\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 3, Valid Accuracy 23.02%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 4, Step 0, Loss 1.3888\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 4, Step 10, Loss 1.3732\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 4, Step 20, Loss 1.3776\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 4, Step 30, Loss 1.3574\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 4, Step 40, Loss 1.3510\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 4, Step 50, Loss 1.3664\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 4, Step 60, Loss 1.3648\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 4, Step 70, Loss 1.4100\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 4, Step 80, Loss 1.3922\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 4, Step 90, Loss 1.3717\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 4, Step 100, Loss 1.3919\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 4, Step 110, Loss 1.3635\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 4, Step 120, Loss 1.3723\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 4, Step 130, Loss 1.3676\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 4, Step 140, Loss 1.3865\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 4, Valid Accuracy 26.19%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m [INFO] Patience 5 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 5, Step 0, Loss 1.3407\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 5, Step 10, Loss 1.3432\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 5, Step 20, Loss 1.3928\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 5, Step 30, Loss 1.3739\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 5, Step 40, Loss 1.3503\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 5, Step 50, Loss 1.3389\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 5, Step 60, Loss 1.4138\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 5, Step 70, Loss 1.3804\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 5, Step 80, Loss 1.3761\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 5, Step 90, Loss 1.3545\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 5, Step 100, Loss 1.4244\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 5, Step 110, Loss 1.4116\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 5, Step 120, Loss 1.3511\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 5, Step 130, Loss 1.3582\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 5, Step 140, Loss 1.3593\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 5, Valid Accuracy 26.19%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m [INFO] Patience 4 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 6, Step 0, Loss 1.3451\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 6, Step 10, Loss 1.4545\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 6, Step 20, Loss 1.3564\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 6, Step 30, Loss 1.3843\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 6, Step 40, Loss 1.4238\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 6, Step 50, Loss 1.5782\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 6, Step 60, Loss 1.4183\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 6, Step 70, Loss 1.3904\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 6, Step 80, Loss 1.3879\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 6, Step 90, Loss 1.4100\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 6, Step 100, Loss 1.3946\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 6, Step 110, Loss 1.3761\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 6, Step 120, Loss 1.3995\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 6, Step 130, Loss 1.3575\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 6, Step 140, Loss 1.4045\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 6, Valid Accuracy 26.19%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m [INFO] Patience 3 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 7, Step 0, Loss 1.4113\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 7, Step 10, Loss 1.3709\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 7, Step 20, Loss 1.3668\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 7, Step 30, Loss 1.4067\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 7, Step 40, Loss 1.4241\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 7, Step 50, Loss 1.3543\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 7, Step 60, Loss 1.4432\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 7, Step 70, Loss 1.4099\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 7, Step 80, Loss 1.3739\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 7, Step 90, Loss 1.3689\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 7, Step 100, Loss 1.4939\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 7, Step 110, Loss 1.3776\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 7, Step 120, Loss 1.3500\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 7, Step 130, Loss 1.4501\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 7, Step 140, Loss 1.4951\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 00008: reducing learning rate of group 0 to 1.0000e-04.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 7, Valid Accuracy 26.19%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m [INFO] Patience 2 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 8, Step 0, Loss 1.3510\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 8, Step 10, Loss 1.4309\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 8, Step 20, Loss 1.3281\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 8, Step 30, Loss 1.3657\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 8, Step 40, Loss 1.4328\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 8, Step 50, Loss 1.3243\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 8, Step 60, Loss 1.3701\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 8, Step 70, Loss 1.4179\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 8, Step 80, Loss 1.4265\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 8, Step 90, Loss 1.4040\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 8, Step 100, Loss 1.3761\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 8, Step 110, Loss 1.3579\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 8, Step 120, Loss 1.3971\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 8, Step 130, Loss 1.3264\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 8, Step 140, Loss 1.3690\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 8, Valid Accuracy 26.19%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m [INFO] Patience 1 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 9, Step 0, Loss 1.3206\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 9, Step 10, Loss 1.4287\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 9, Step 20, Loss 1.3762\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 9, Step 30, Loss 1.3693\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 9, Step 40, Loss 1.3967\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 9, Step 50, Loss 1.3264\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 9, Step 60, Loss 1.3891\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 9, Step 70, Loss 1.3898\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 9, Step 80, Loss 1.3244\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 9, Step 90, Loss 1.3798\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 9, Step 100, Loss 1.4330\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 9, Step 110, Loss 1.3988\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 9, Step 120, Loss 1.3969\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 9, Step 130, Loss 1.4501\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 9, Step 140, Loss 1.3859\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m Epoch 9, Valid Accuracy 26.19%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m [Early Stopping] Epoch 9, Valid Accuracy 26.19%, Valid Loss 1.3925\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=998)\u001b[0m [Client 0] get_parameters\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m [Client 1] fit, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m /opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 0, Step 0, Loss 1.4101\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 0, Step 10, Loss 1.3862\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 0, Step 20, Loss 1.4075\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 0, Step 30, Loss 1.3651\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 0, Step 40, Loss 1.3764\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 0, Step 50, Loss 1.3841\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 0, Step 60, Loss 1.4324\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 0, Step 70, Loss 1.3815\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 0, Step 80, Loss 1.4050\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 0, Step 90, Loss 1.3795\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 0, Step 100, Loss 1.3746\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 0, Step 110, Loss 1.4132\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 0, Step 120, Loss 1.3795\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 0, Step 130, Loss 1.3835\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 0, Step 140, Loss 1.3874\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 0, Valid Accuracy 26.98%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 1, Step 0, Loss 1.4206\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 1, Step 10, Loss 1.3730\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 1, Step 20, Loss 1.4079\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 1, Step 30, Loss 1.3669\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 1, Step 40, Loss 1.3850\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 1, Step 50, Loss 1.3830\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 1, Step 60, Loss 1.3923\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 1, Step 70, Loss 1.4011\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 1, Step 80, Loss 1.3974\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 1, Step 90, Loss 1.3869\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 1, Step 100, Loss 1.4230\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 1, Step 110, Loss 1.3815\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 1, Step 120, Loss 1.3866\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 1, Step 130, Loss 1.3963\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 1, Step 140, Loss 1.4133\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 1, Valid Accuracy 17.46%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 2, Step 0, Loss 1.4092\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 2, Step 10, Loss 1.3623\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 2, Step 20, Loss 1.3510\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 2, Step 30, Loss 1.4351\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 2, Step 40, Loss 1.4050\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 2, Step 50, Loss 1.4138\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 2, Step 60, Loss 1.4018\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 2, Step 70, Loss 1.3702\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 2, Step 80, Loss 1.3792\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 2, Step 90, Loss 1.3929\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 2, Step 100, Loss 1.3598\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 2, Step 110, Loss 1.4135\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 2, Step 120, Loss 1.4027\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 2, Step 130, Loss 1.4046\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 2, Step 140, Loss 1.3827\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 2, Valid Accuracy 26.98%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m [INFO] Patience 5 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 3, Step 0, Loss 1.3862\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 3, Step 10, Loss 1.3628\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 3, Step 20, Loss 1.3539\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 3, Step 30, Loss 1.3754\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 3, Step 40, Loss 1.4023\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 3, Step 50, Loss 1.3780\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 3, Step 60, Loss 1.3698\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 3, Step 70, Loss 1.3665\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 3, Step 80, Loss 1.3613\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 3, Step 90, Loss 1.3896\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 3, Step 100, Loss 1.4032\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 3, Step 110, Loss 1.3737\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 3, Step 120, Loss 1.3643\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 3, Step 130, Loss 1.3799\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 3, Step 140, Loss 1.4092\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 3, Valid Accuracy 17.46%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m [INFO] Patience 4 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 4, Step 0, Loss 1.3896\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 4, Step 10, Loss 1.3739\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 4, Step 20, Loss 1.3718\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 4, Step 30, Loss 1.3885\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 4, Step 40, Loss 1.3857\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 4, Step 50, Loss 1.3443\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 4, Step 60, Loss 1.4326\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 4, Step 70, Loss 1.3694\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 4, Step 80, Loss 1.3836\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 4, Step 90, Loss 1.3956\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 4, Step 100, Loss 1.3450\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 4, Step 110, Loss 1.3579\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 4, Step 120, Loss 1.3908\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 4, Step 130, Loss 1.4304\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 4, Step 140, Loss 1.4393\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 4, Valid Accuracy 26.98%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m [INFO] Patience 3 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 5, Step 0, Loss 1.4255\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 5, Step 10, Loss 1.4010\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 5, Step 20, Loss 1.3853\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 5, Step 30, Loss 1.3710\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 5, Step 40, Loss 1.3944\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 5, Step 50, Loss 1.4062\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 5, Step 60, Loss 1.3918\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 5, Step 70, Loss 1.3738\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 5, Step 80, Loss 1.3824\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 5, Step 90, Loss 1.3987\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 5, Step 100, Loss 1.3892\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 5, Step 110, Loss 1.3557\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 5, Step 120, Loss 1.3647\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 5, Step 130, Loss 1.4042\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 5, Step 140, Loss 1.4097\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 00006: reducing learning rate of group 0 to 1.0000e-04.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 5, Valid Accuracy 26.98%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m [INFO] Patience 2 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 6, Step 0, Loss 1.3675\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 6, Step 10, Loss 1.3852\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 6, Step 20, Loss 1.3669\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 6, Step 30, Loss 1.3618\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 6, Step 40, Loss 1.3786\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 6, Step 50, Loss 1.3824\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 6, Step 60, Loss 1.3657\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 6, Step 70, Loss 1.4344\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 6, Step 80, Loss 1.3822\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 6, Step 90, Loss 1.3816\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 6, Step 100, Loss 1.3678\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 6, Step 110, Loss 1.4017\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 6, Step 120, Loss 1.3989\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 6, Step 130, Loss 1.3547\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 6, Step 140, Loss 1.3890\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 6, Valid Accuracy 26.98%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m [INFO] Patience 1 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 7, Step 0, Loss 1.3847\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 7, Step 10, Loss 1.3795\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 7, Step 20, Loss 1.4028\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 7, Step 30, Loss 1.3677\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 7, Step 40, Loss 1.3683\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 7, Step 50, Loss 1.3907\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 7, Step 60, Loss 1.3974\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 7, Step 70, Loss 1.3866\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 7, Step 80, Loss 1.3616\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 7, Step 90, Loss 1.4046\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 7, Step 100, Loss 1.3989\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 7, Step 110, Loss 1.4073\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 7, Step 120, Loss 1.3996\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 7, Step 130, Loss 1.3459\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 7, Step 140, Loss 1.3630\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m Epoch 7, Valid Accuracy 26.98%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m [Early Stopping] Epoch 7, Valid Accuracy 26.98%, Valid Loss 1.3937\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1263)\u001b[0m [Client 1] get_parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flower 2023-12-12 02:11:08,112 | server.py:277 | fit_round received 2 results and 0 failures\n",
      "INFO flower 2023-12-12 02:11:23,472 | server.py:158 | fit progress: (3, 0.17622614474523635, {'val_accuracy': 0.23015873015873015, 'test_accuracy': 0.26303317535545023}, 1467.155585556)\n",
      "INFO flower 2023-12-12 02:11:23,474 | server.py:209 | evaluate_round: no clients selected, cancel\n",
      "DEBUG flower 2023-12-12 02:11:23,475 | server.py:265 | fit_round: strategy sampled 2 clients (out of 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[1m\u001b[36m(scheduler +25m21s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n",
      "\u001b[2m\u001b[1m\u001b[33m(scheduler +25m21s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 4.0, 'GPU': 2.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m [Client 2] fit, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m /opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 0, Step 0, Loss 1.3931\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 0, Step 10, Loss 1.3886\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 0, Step 20, Loss 1.4008\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 0, Step 30, Loss 1.4007\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 0, Step 40, Loss 1.3766\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 0, Step 50, Loss 1.3815\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 0, Step 60, Loss 1.3871\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 0, Step 70, Loss 1.3686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(raylet)\u001b[0m Spilled 7680 MiB, 14 objects, write throughput 230 MiB/s. Set RAY_verbose_spill_logs=0 to disable this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 0, Step 80, Loss 1.3672\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 0, Step 90, Loss 1.3664\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 0, Step 100, Loss 1.4400\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 0, Step 110, Loss 1.3587\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 0, Step 120, Loss 1.4688\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 0, Step 130, Loss 1.5106\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 0, Step 140, Loss 1.4225\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 0, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 1, Step 0, Loss 1.4196\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 1, Step 10, Loss 1.3738\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 1, Step 20, Loss 1.3994\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 1, Step 30, Loss 1.3867\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 1, Step 40, Loss 1.3826\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 1, Step 50, Loss 1.3790\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 1, Step 60, Loss 1.3825\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 1, Step 70, Loss 1.3892\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 1, Step 80, Loss 1.3713\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 1, Step 90, Loss 1.3935\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 1, Step 100, Loss 1.3936\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 1, Step 110, Loss 1.3659\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 1, Step 120, Loss 1.4068\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 1, Step 130, Loss 1.4236\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 1, Step 140, Loss 1.3993\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 1, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m [INFO] Patience 5 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 2, Step 0, Loss 1.3776\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 2, Step 10, Loss 1.3832\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 2, Step 20, Loss 1.4164\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 2, Step 30, Loss 1.3516\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 2, Step 40, Loss 1.3514\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 2, Step 50, Loss 1.3790\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 2, Step 60, Loss 1.3906\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 2, Step 70, Loss 1.4211\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 2, Step 80, Loss 1.4384\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 2, Step 90, Loss 1.4165\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 2, Step 100, Loss 1.3658\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 2, Step 110, Loss 1.3842\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 2, Step 120, Loss 1.4021\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 2, Step 130, Loss 1.3844\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 2, Step 140, Loss 1.3510\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 2, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m [INFO] Patience 4 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 3, Step 0, Loss 1.3812\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 3, Step 10, Loss 1.3386\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 3, Step 20, Loss 1.3569\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 3, Step 30, Loss 1.4005\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 3, Step 40, Loss 1.4255\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 3, Step 50, Loss 1.3976\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 3, Step 60, Loss 1.3707\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 3, Step 70, Loss 1.4351\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 3, Step 80, Loss 1.3695\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 3, Step 90, Loss 1.4473\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 3, Step 100, Loss 1.3700\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 3, Step 110, Loss 1.4080\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 3, Step 120, Loss 1.3804\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 3, Step 130, Loss 1.3894\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 3, Step 140, Loss 1.3682\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 3, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m [INFO] Patience 3 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 4, Step 0, Loss 1.3505\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 4, Step 10, Loss 1.3878\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 4, Step 20, Loss 1.3755\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 4, Step 30, Loss 1.3797\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 4, Step 40, Loss 1.3412\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 4, Step 50, Loss 1.3779\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 4, Step 60, Loss 1.4317\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 4, Step 70, Loss 1.4038\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 4, Step 80, Loss 1.3042\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 4, Step 90, Loss 1.3728\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 4, Step 100, Loss 1.3880\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 4, Step 110, Loss 1.3570\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 4, Step 120, Loss 1.3850\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 4, Step 130, Loss 1.3776\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 4, Step 140, Loss 1.3508\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 00005: reducing learning rate of group 0 to 1.0000e-04.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 4, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m [INFO] Patience 2 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 5, Step 0, Loss 1.4061\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 5, Step 10, Loss 1.4056\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 5, Step 20, Loss 1.3732\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 5, Step 30, Loss 1.3778\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 5, Step 40, Loss 1.3503\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 5, Step 50, Loss 1.3763\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 5, Step 60, Loss 1.3659\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 5, Step 70, Loss 1.3477\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 5, Step 80, Loss 1.3630\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 5, Step 90, Loss 1.3639\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 5, Step 100, Loss 1.3636\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 5, Step 110, Loss 1.4095\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 5, Step 120, Loss 1.3754\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 5, Step 130, Loss 1.3501\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 5, Step 140, Loss 1.3977\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 5, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m [INFO] Patience 1 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 6, Step 0, Loss 1.4420\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 6, Step 10, Loss 1.3292\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 6, Step 20, Loss 1.3658\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 6, Step 30, Loss 1.4160\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 6, Step 40, Loss 1.3542\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 6, Step 50, Loss 1.3769\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 6, Step 60, Loss 1.4342\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 6, Step 70, Loss 1.4008\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 6, Step 80, Loss 1.3933\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 6, Step 90, Loss 1.3501\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 6, Step 100, Loss 1.3803\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 6, Step 110, Loss 1.3622\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 6, Step 120, Loss 1.4066\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 6, Step 130, Loss 1.3571\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 6, Step 140, Loss 1.3890\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m Epoch 6, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m [Early Stopping] Epoch 6, Valid Accuracy 19.84%, Valid Loss 1.3968\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1486)\u001b[0m [Client 2] get_parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=1780)\u001b[0m 2023-12-12 02:15:38,496\tERROR serialization.py:342 -- CUDA error: out of memory\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1780)\u001b[0m CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1780)\u001b[0m For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1780)\u001b[0m Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1780)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1780)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/ray/serialization.py\", line 340, in deserialize_objects\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1780)\u001b[0m     obj = self._deserialize_object(data, metadata, object_ref)\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1780)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/ray/serialization.py\", line 237, in _deserialize_object\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1780)\u001b[0m     return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1780)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/ray/serialization.py\", line 192, in _deserialize_msgpack_data\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1780)\u001b[0m     python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1780)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/ray/serialization.py\", line 182, in _deserialize_pickle5_data\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1780)\u001b[0m     obj = pickle.loads(in_band)\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1780)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1780)\u001b[0m     return torch.load(io.BytesIO(b))\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1780)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1780)\u001b[0m     return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1780)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1780)\u001b[0m     result = unpickler.load()\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1780)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1780)\u001b[0m     wrap_storage=restore_location(obj, location),\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1780)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1780)\u001b[0m     result = fn(storage, location)\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1780)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1780)\u001b[0m     return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1780)\u001b[0m RuntimeError: CUDA error: out of memory\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1780)\u001b[0m CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1780)\u001b[0m For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1780)\u001b[0m Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1780)\u001b[0m \n",
      "DEBUG flower 2023-12-12 02:15:38,993 | server.py:277 | fit_round received 1 results and 1 failures\n",
      "INFO flower 2023-12-12 02:15:59,350 | server.py:158 | fit progress: (4, 0.17649534486588977, {'val_accuracy': 0.25396825396825395, 'test_accuracy': 0.26540284360189575}, 1743.033329686)\n",
      "INFO flower 2023-12-12 02:15:59,351 | server.py:209 | evaluate_round: no clients selected, cancel\n",
      "DEBUG flower 2023-12-12 02:15:59,352 | server.py:265 | fit_round: strategy sampled 2 clients (out of 3)\n",
      "\u001b[2m\u001b[36m(raylet)\u001b[0m Spilled 8777 MiB, 16 objects, write throughput 245 MiB/s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m [Client 1] fit, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m /opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 0, Step 0, Loss 1.3614\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 0, Step 10, Loss 1.3658\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 0, Step 20, Loss 1.3797\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 0, Step 30, Loss 1.3774\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 0, Step 40, Loss 1.3839\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 0, Step 50, Loss 1.3909\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 0, Step 60, Loss 1.3763\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 0, Step 70, Loss 1.3806\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 0, Step 80, Loss 1.4234\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 0, Step 90, Loss 1.4293\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 0, Step 100, Loss 1.3812\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 0, Step 110, Loss 1.3549\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 0, Step 120, Loss 1.4191\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 0, Step 130, Loss 1.3565\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 0, Step 140, Loss 1.4233\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 0, Valid Accuracy 17.46%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 1, Step 0, Loss 1.4135\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 1, Step 10, Loss 1.3482\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 1, Step 20, Loss 1.4036\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 1, Step 30, Loss 1.3788\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 1, Step 40, Loss 1.3731\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 1, Step 50, Loss 1.4130\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 1, Step 60, Loss 1.3877\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 1, Step 70, Loss 1.3721\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 1, Step 80, Loss 1.3611\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 1, Step 90, Loss 1.3923\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 1, Step 100, Loss 1.4409\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 1, Step 110, Loss 1.4023\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 1, Step 120, Loss 1.4164\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 1, Step 130, Loss 1.3609\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 1, Step 140, Loss 1.4290\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 1, Valid Accuracy 17.46%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 2, Step 0, Loss 1.3605\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 2, Step 10, Loss 1.3725\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 2, Step 20, Loss 1.3902\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 2, Step 30, Loss 1.3714\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 2, Step 40, Loss 1.3791\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 2, Step 50, Loss 1.3945\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 2, Step 60, Loss 1.3788\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 2, Step 70, Loss 1.3834\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 2, Step 80, Loss 1.3889\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 2, Step 90, Loss 1.3687\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 2, Step 100, Loss 1.3966\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 2, Step 110, Loss 1.3567\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 2, Step 120, Loss 1.3606\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 2, Step 130, Loss 1.3475\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 2, Step 140, Loss 1.3864\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 2, Valid Accuracy 17.46%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m [INFO] Patience 5 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 3, Step 0, Loss 1.3754\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 3, Step 10, Loss 1.4127\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 3, Step 20, Loss 1.3842\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 3, Step 30, Loss 1.3706\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 3, Step 40, Loss 1.3865\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 3, Step 50, Loss 1.3813\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 3, Step 60, Loss 1.3819\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 3, Step 70, Loss 1.3804\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 3, Step 80, Loss 1.3918\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 3, Step 90, Loss 1.4062\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 3, Step 100, Loss 1.3437\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 3, Step 110, Loss 1.4516\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 3, Step 120, Loss 1.3973\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 3, Step 130, Loss 1.3616\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 3, Step 140, Loss 1.3523\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 3, Valid Accuracy 17.46%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m [INFO] Patience 4 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 4, Step 0, Loss 1.3871\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 4, Step 10, Loss 1.3353\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 4, Step 20, Loss 1.4129\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 4, Step 30, Loss 1.3490\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 4, Step 40, Loss 1.3791\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 4, Step 50, Loss 1.3770\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 4, Step 60, Loss 1.3779\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 4, Step 70, Loss 1.4056\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 4, Step 80, Loss 1.3630\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 4, Step 90, Loss 1.3688\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 4, Step 100, Loss 1.3658\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 4, Step 110, Loss 1.3594\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 4, Step 120, Loss 1.4133\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 4, Step 130, Loss 1.3933\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 4, Step 140, Loss 1.3869\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 4, Valid Accuracy 17.46%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m [INFO] Patience 3 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 5, Step 0, Loss 1.3883\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 5, Step 10, Loss 1.4119\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 5, Step 20, Loss 1.3845\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 5, Step 30, Loss 1.4200\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 5, Step 40, Loss 1.3849\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 5, Step 50, Loss 1.4197\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 5, Step 60, Loss 1.3320\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 5, Step 70, Loss 1.3939\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 5, Step 80, Loss 1.3975\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 5, Step 90, Loss 1.3899\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 5, Step 100, Loss 1.3695\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 5, Step 110, Loss 1.3596\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 5, Step 120, Loss 1.4009\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 5, Step 130, Loss 1.3723\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 5, Step 140, Loss 1.3917\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 00006: reducing learning rate of group 0 to 1.0000e-04.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 5, Valid Accuracy 26.98%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m [INFO] Patience 2 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 6, Step 0, Loss 1.3788\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 6, Step 10, Loss 1.3693\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 6, Step 20, Loss 1.3885\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 6, Step 30, Loss 1.3973\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 6, Step 40, Loss 1.3587\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 6, Step 50, Loss 1.4186\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 6, Step 60, Loss 1.4106\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 6, Step 70, Loss 1.4172\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 6, Step 80, Loss 1.3776\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 6, Step 90, Loss 1.3901\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 6, Step 100, Loss 1.4164\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 6, Step 110, Loss 1.3963\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 6, Step 120, Loss 1.3848\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 6, Step 130, Loss 1.3576\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 6, Step 140, Loss 1.4012\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 6, Valid Accuracy 26.98%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 7, Step 0, Loss 1.3875\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 7, Step 10, Loss 1.3844\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 7, Step 20, Loss 1.3596\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 7, Step 30, Loss 1.3471\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 7, Step 40, Loss 1.4262\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 7, Step 50, Loss 1.3737\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 7, Step 60, Loss 1.3959\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 7, Step 70, Loss 1.3793\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 7, Step 80, Loss 1.4314\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 7, Step 90, Loss 1.4050\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 7, Step 100, Loss 1.3583\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 7, Step 110, Loss 1.3793\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 7, Step 120, Loss 1.4220\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 7, Step 130, Loss 1.3561\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 7, Step 140, Loss 1.4218\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 7, Valid Accuracy 26.98%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m [INFO] Patience 5 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 8, Step 0, Loss 1.3975\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 8, Step 10, Loss 1.3972\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 8, Step 20, Loss 1.4180\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 8, Step 30, Loss 1.4165\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 8, Step 40, Loss 1.3886\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 8, Step 50, Loss 1.3518\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 8, Step 60, Loss 1.4246\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 8, Step 70, Loss 1.3795\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 8, Step 80, Loss 1.3666\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 8, Step 90, Loss 1.3874\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 8, Step 100, Loss 1.3894\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 8, Step 110, Loss 1.4322\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 8, Step 120, Loss 1.3376\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 8, Step 130, Loss 1.4256\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 8, Step 140, Loss 1.3666\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 8, Valid Accuracy 26.98%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m [INFO] Patience 4 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 9, Step 0, Loss 1.4270\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 9, Step 10, Loss 1.3975\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 9, Step 20, Loss 1.4001\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 9, Step 30, Loss 1.4010\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 9, Step 40, Loss 1.3897\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 9, Step 50, Loss 1.4281\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 9, Step 60, Loss 1.3888\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 9, Step 70, Loss 1.3991\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 9, Step 80, Loss 1.3621\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 9, Step 90, Loss 1.3548\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 9, Step 100, Loss 1.3931\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 9, Step 110, Loss 1.3805\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 9, Step 120, Loss 1.4098\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 9, Step 130, Loss 1.3908\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 9, Step 140, Loss 1.3588\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 9, Valid Accuracy 26.98%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m [INFO] Patience 3 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 10, Step 0, Loss 1.3811\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 10, Step 10, Loss 1.3824\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 10, Step 20, Loss 1.4026\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 10, Step 30, Loss 1.3882\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 10, Step 40, Loss 1.3671\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 10, Step 50, Loss 1.4186\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 10, Step 60, Loss 1.3263\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 10, Step 70, Loss 1.3791\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 10, Step 80, Loss 1.4409\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 10, Step 90, Loss 1.3693\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 10, Step 100, Loss 1.3674\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 10, Step 110, Loss 1.3547\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 10, Step 120, Loss 1.3915\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 10, Step 130, Loss 1.3875\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 10, Step 140, Loss 1.3962\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 00011: reducing learning rate of group 0 to 1.0000e-05.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 10, Valid Accuracy 26.98%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m [INFO] Patience 2 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 11, Step 0, Loss 1.3491\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 11, Step 10, Loss 1.3592\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 11, Step 20, Loss 1.3876\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 11, Step 30, Loss 1.3569\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 11, Step 40, Loss 1.4387\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 11, Step 50, Loss 1.3863\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 11, Step 60, Loss 1.3478\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 11, Step 70, Loss 1.3698\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 11, Step 80, Loss 1.3876\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 11, Step 90, Loss 1.3704\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 11, Step 100, Loss 1.3782\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 11, Step 110, Loss 1.3981\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 11, Step 120, Loss 1.3855\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 11, Step 130, Loss 1.4099\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 11, Step 140, Loss 1.3647\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 11, Valid Accuracy 26.98%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m [INFO] Patience 1 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 12, Step 0, Loss 1.3579\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 12, Step 10, Loss 1.3859\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 12, Step 20, Loss 1.4010\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 12, Step 30, Loss 1.3526\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 12, Step 40, Loss 1.3817\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 12, Step 50, Loss 1.3698\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 12, Step 60, Loss 1.3672\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 12, Step 70, Loss 1.3706\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 12, Step 80, Loss 1.4180\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 12, Step 90, Loss 1.3962\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 12, Step 100, Loss 1.3882\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 12, Step 110, Loss 1.3801\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 12, Step 120, Loss 1.3674\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 12, Step 130, Loss 1.3696\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 12, Step 140, Loss 1.3662\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m Epoch 12, Valid Accuracy 26.98%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m [Early Stopping] Epoch 12, Valid Accuracy 26.98%, Valid Loss 1.3958\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=1818)\u001b[0m [Client 1] get_parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(raylet)\u001b[0m Spilled 9326 MiB, 17 objects, write throughput 252 MiB/s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m [Client 2] fit, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m /opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 0, Step 0, Loss 1.3456\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 0, Step 10, Loss 1.3807\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 0, Step 20, Loss 1.3604\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 0, Step 30, Loss 1.3589\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 0, Step 40, Loss 1.4035\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 0, Step 50, Loss 1.4280\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 0, Step 60, Loss 1.4511\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 0, Step 70, Loss 1.3907\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 0, Step 80, Loss 1.3875\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 0, Step 90, Loss 1.4195\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 0, Step 100, Loss 1.3858\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 0, Step 110, Loss 1.4087\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 0, Step 120, Loss 1.3604\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 0, Step 130, Loss 1.3606\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 0, Step 140, Loss 1.3663\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 0, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 1, Step 0, Loss 1.3661\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 1, Step 10, Loss 1.3722\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 1, Step 20, Loss 1.3539\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 1, Step 30, Loss 1.3654\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 1, Step 40, Loss 1.4164\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 1, Step 50, Loss 1.3559\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 1, Step 60, Loss 1.3870\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 1, Step 70, Loss 1.3617\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 1, Step 80, Loss 1.3738\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 1, Step 90, Loss 1.3947\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 1, Step 100, Loss 1.4100\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 1, Step 110, Loss 1.4411\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 1, Step 120, Loss 1.3674\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 1, Step 130, Loss 1.3835\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 1, Step 140, Loss 1.3748\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 1, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m [INFO] Patience 5 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 2, Step 0, Loss 1.3978\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 2, Step 10, Loss 1.3591\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 2, Step 20, Loss 1.3759\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 2, Step 30, Loss 1.4196\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 2, Step 40, Loss 1.4256\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 2, Step 50, Loss 1.3403\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 2, Step 60, Loss 1.3795\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 2, Step 70, Loss 1.4124\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 2, Step 80, Loss 1.3668\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 2, Step 90, Loss 1.3944\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 2, Step 100, Loss 1.4364\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 2, Step 110, Loss 1.3813\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 2, Step 120, Loss 1.3655\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 2, Step 130, Loss 1.3532\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 2, Step 140, Loss 1.3636\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 2, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m [INFO] Patience 4 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 3, Step 0, Loss 1.4113\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 3, Step 10, Loss 1.3661\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 3, Step 20, Loss 1.4353\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 3, Step 30, Loss 1.3597\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 3, Step 40, Loss 1.3935\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 3, Step 50, Loss 1.3411\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 3, Step 60, Loss 1.3816\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 3, Step 70, Loss 1.2861\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 3, Step 80, Loss 1.3745\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 3, Step 90, Loss 1.3662\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 3, Step 100, Loss 1.3732\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 3, Step 110, Loss 1.3572\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 3, Step 120, Loss 1.3431\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 3, Step 130, Loss 1.3222\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 3, Step 140, Loss 1.4220\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 3, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 4, Step 0, Loss 1.3902\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 4, Step 10, Loss 1.4309\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 4, Step 20, Loss 1.3748\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 4, Step 30, Loss 1.3533\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 4, Step 40, Loss 1.4095\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 4, Step 50, Loss 1.3807\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 4, Step 60, Loss 1.3948\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 4, Step 70, Loss 1.3933\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 4, Step 80, Loss 1.3637\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 4, Step 90, Loss 1.3540\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 4, Step 100, Loss 1.3682\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 4, Step 110, Loss 1.4461\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 4, Step 120, Loss 1.3903\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 4, Step 130, Loss 1.4254\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 4, Step 140, Loss 1.3424\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 4, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m [INFO] Patience 5 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 5, Step 0, Loss 1.4237\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 5, Step 10, Loss 1.3578\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 5, Step 20, Loss 1.3799\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 5, Step 30, Loss 1.4088\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 5, Step 40, Loss 1.4121\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 5, Step 50, Loss 1.3633\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 5, Step 60, Loss 1.3672\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 5, Step 70, Loss 1.4100\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 5, Step 80, Loss 1.4077\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 5, Step 90, Loss 1.4033\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 5, Step 100, Loss 1.3895\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 5, Step 110, Loss 1.3728\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 5, Step 120, Loss 1.3646\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 5, Step 130, Loss 1.3905\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 5, Step 140, Loss 1.4185\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 5, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m [INFO] Patience 4 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 6, Step 0, Loss 1.3627\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 6, Step 10, Loss 1.3863\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 6, Step 20, Loss 1.3878\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 6, Step 30, Loss 1.4024\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 6, Step 40, Loss 1.4316\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 6, Step 50, Loss 1.4503\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 6, Step 60, Loss 1.4759\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 6, Step 70, Loss 1.3715\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 6, Step 80, Loss 1.3704\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 6, Step 90, Loss 1.3882\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 6, Step 100, Loss 1.3275\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 6, Step 110, Loss 1.4213\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 6, Step 120, Loss 1.4103\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 6, Step 130, Loss 1.3755\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 6, Step 140, Loss 1.3501\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 6, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 7, Step 0, Loss 1.4091\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 7, Step 10, Loss 1.4144\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 7, Step 20, Loss 1.3976\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 7, Step 30, Loss 1.3409\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 7, Step 40, Loss 1.3460\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 7, Step 50, Loss 1.3723\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 7, Step 60, Loss 1.5399\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 7, Step 70, Loss 1.4365\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 7, Step 80, Loss 1.3790\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 7, Step 90, Loss 1.3727\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 7, Step 100, Loss 1.3786\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 7, Step 110, Loss 1.3944\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 7, Step 120, Loss 1.3597\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 7, Step 130, Loss 1.4510\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 7, Step 140, Loss 1.4426\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 7, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m [INFO] Patience 5 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 8, Step 0, Loss 1.3414\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 8, Step 10, Loss 1.3660\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 8, Step 20, Loss 1.3818\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 8, Step 30, Loss 1.3623\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 8, Step 40, Loss 1.3582\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 8, Step 50, Loss 1.4247\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 8, Step 60, Loss 1.3904\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 8, Step 70, Loss 1.3980\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 8, Step 80, Loss 1.3953\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 8, Step 90, Loss 1.3687\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 8, Step 100, Loss 1.3751\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 8, Step 110, Loss 1.4072\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 8, Step 120, Loss 1.3874\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 8, Step 130, Loss 1.4071\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 8, Step 140, Loss 1.3846\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 8, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m [INFO] Patience 4 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 9, Step 0, Loss 1.3787\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 9, Step 10, Loss 1.3446\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 9, Step 20, Loss 1.3862\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 9, Step 30, Loss 1.4247\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 9, Step 40, Loss 1.3807\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 9, Step 50, Loss 1.3662\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 9, Step 60, Loss 1.4510\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 9, Step 70, Loss 1.3715\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 9, Step 80, Loss 1.3744\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 9, Step 90, Loss 1.3694\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 9, Step 100, Loss 1.3611\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 9, Step 110, Loss 1.3586\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 9, Step 120, Loss 1.3825\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 9, Step 130, Loss 1.3518\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 9, Step 140, Loss 1.3590\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 9, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m [INFO] Patience 3 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 10, Step 0, Loss 1.3542\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 10, Step 10, Loss 1.3818\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 10, Step 20, Loss 1.3088\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 10, Step 30, Loss 1.4029\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 10, Step 40, Loss 1.3132\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 10, Step 50, Loss 1.3612\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 10, Step 60, Loss 1.3858\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 10, Step 70, Loss 1.3512\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 10, Step 80, Loss 1.4573\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 10, Step 90, Loss 1.3874\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 10, Step 100, Loss 1.3709\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 10, Step 110, Loss 1.4252\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 10, Step 120, Loss 1.4060\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 10, Step 130, Loss 1.4021\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 10, Step 140, Loss 1.3772\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 10, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m [INFO] Patience 2 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 11, Step 0, Loss 1.3709\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 11, Step 10, Loss 1.3537\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 11, Step 20, Loss 1.3906\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 11, Step 30, Loss 1.3889\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 11, Step 40, Loss 1.4111\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 11, Step 50, Loss 1.3933\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 11, Step 60, Loss 1.3799\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 11, Step 70, Loss 1.3934\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 11, Step 80, Loss 1.3656\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 11, Step 90, Loss 1.3837\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 11, Step 100, Loss 1.3732\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 11, Step 110, Loss 1.4008\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 11, Step 120, Loss 1.3744\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 11, Step 130, Loss 1.3750\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 11, Step 140, Loss 1.3722\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 11, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m [INFO] Patience 1 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 12, Step 0, Loss 1.3940\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 12, Step 10, Loss 1.3843\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 12, Step 20, Loss 1.3864\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 12, Step 30, Loss 1.3530\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 12, Step 40, Loss 1.3903\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 12, Step 50, Loss 1.3799\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 12, Step 60, Loss 1.3822\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 12, Step 70, Loss 1.3775\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 12, Step 80, Loss 1.4057\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 12, Step 90, Loss 1.4017\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 12, Step 100, Loss 1.3866\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 12, Step 110, Loss 1.4246\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 12, Step 120, Loss 1.4078\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 12, Step 130, Loss 1.4027\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 12, Step 140, Loss 1.3895\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m Epoch 12, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m [Early Stopping] Epoch 12, Valid Accuracy 19.84%, Valid Loss 1.3948\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2154)\u001b[0m [Client 2] get_parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flower 2023-12-12 02:30:22,988 | server.py:277 | fit_round received 2 results and 0 failures\n",
      "INFO flower 2023-12-12 02:30:42,912 | server.py:158 | fit progress: (5, 0.17634660005569458, {'val_accuracy': 0.25396825396825395, 'test_accuracy': 0.26540284360189575}, 2626.595578198)\n",
      "INFO flower 2023-12-12 02:30:42,913 | server.py:209 | evaluate_round: no clients selected, cancel\n",
      "DEBUG flower 2023-12-12 02:30:42,914 | server.py:265 | fit_round: strategy sampled 2 clients (out of 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m [Client 2] fit, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m /opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 0, Step 0, Loss 1.3601\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 0, Step 10, Loss 1.4136\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 0, Step 20, Loss 1.3868\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 0, Step 30, Loss 1.3938\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 0, Step 40, Loss 1.3499\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 0, Step 50, Loss 1.3755\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 0, Step 60, Loss 1.4092\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 0, Step 70, Loss 1.3801\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 0, Step 80, Loss 1.3773\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 0, Step 90, Loss 1.3853\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 0, Step 100, Loss 1.3696\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 0, Step 110, Loss 1.3547\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 0, Step 120, Loss 1.4208\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 0, Step 130, Loss 1.3736\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 0, Step 140, Loss 1.3748\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 0, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 1, Step 0, Loss 1.3790\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 1, Step 10, Loss 1.4102\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 1, Step 20, Loss 1.3609\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 1, Step 30, Loss 1.4009\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 1, Step 40, Loss 1.3617\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 1, Step 50, Loss 1.3625\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 1, Step 60, Loss 1.3451\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 1, Step 70, Loss 1.3702\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 1, Step 80, Loss 1.3536\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 1, Step 90, Loss 1.3820\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 1, Step 100, Loss 1.4118\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 1, Step 110, Loss 1.3889\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 1, Step 120, Loss 1.3811\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 1, Step 130, Loss 1.4197\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 1, Step 140, Loss 1.3738\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 1, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m [INFO] Patience 5 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 2, Step 0, Loss 1.3722\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 2, Step 10, Loss 1.3541\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 2, Step 20, Loss 1.3395\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 2, Step 30, Loss 1.3236\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 2, Step 40, Loss 1.4394\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 2, Step 50, Loss 1.4516\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 2, Step 60, Loss 1.3426\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 2, Step 70, Loss 1.4070\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 2, Step 80, Loss 1.3940\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 2, Step 90, Loss 1.3718\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 2, Step 100, Loss 1.4157\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 2, Step 110, Loss 1.3577\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 2, Step 120, Loss 1.3480\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 2, Step 130, Loss 1.4073\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 2, Step 140, Loss 1.3818\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 2, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m [INFO] Patience 4 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 3, Step 0, Loss 1.3936\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 3, Step 10, Loss 1.3709\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 3, Step 20, Loss 1.3319\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 3, Step 30, Loss 1.4031\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 3, Step 40, Loss 1.4300\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 3, Step 50, Loss 1.3985\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 3, Step 60, Loss 1.3973\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 3, Step 70, Loss 1.3722\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 3, Step 80, Loss 1.4179\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 3, Step 90, Loss 1.3332\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 3, Step 100, Loss 1.4087\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 3, Step 110, Loss 1.4163\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 3, Step 120, Loss 1.3628\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 3, Step 130, Loss 1.3791\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 3, Step 140, Loss 1.4149\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 3, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m [INFO] Patience 3 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 4, Step 0, Loss 1.3239\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 4, Step 10, Loss 1.3876\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 4, Step 20, Loss 1.3314\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 4, Step 30, Loss 1.3435\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 4, Step 40, Loss 1.4378\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 4, Step 50, Loss 1.3842\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 4, Step 60, Loss 1.3762\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 4, Step 70, Loss 1.3733\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 4, Step 80, Loss 1.3166\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 4, Step 90, Loss 1.4266\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 4, Step 100, Loss 1.4249\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 4, Step 110, Loss 1.4190\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 4, Step 120, Loss 1.3548\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 4, Step 130, Loss 1.3893\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 4, Step 140, Loss 1.3627\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 4, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 5, Step 0, Loss 1.3715\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 5, Step 10, Loss 1.4145\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 5, Step 20, Loss 1.3506\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 5, Step 30, Loss 1.4001\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 5, Step 40, Loss 1.3776\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 5, Step 50, Loss 1.3620\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 5, Step 60, Loss 1.4238\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 5, Step 70, Loss 1.4400\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 5, Step 80, Loss 1.3778\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 5, Step 90, Loss 1.3843\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 5, Step 100, Loss 1.3686\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 5, Step 110, Loss 1.3542\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 5, Step 120, Loss 1.4310\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 5, Step 130, Loss 1.3650\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 5, Step 140, Loss 1.3918\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 5, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m [INFO] Patience 5 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 6, Step 0, Loss 1.3730\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 6, Step 10, Loss 1.3930\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 6, Step 20, Loss 1.4188\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 6, Step 30, Loss 1.4197\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 6, Step 40, Loss 1.4390\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 6, Step 50, Loss 1.3822\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 6, Step 60, Loss 1.3564\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 6, Step 70, Loss 1.3749\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 6, Step 80, Loss 1.3998\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 6, Step 90, Loss 1.3835\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 6, Step 100, Loss 1.3531\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 6, Step 110, Loss 1.3751\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 6, Step 120, Loss 1.3684\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 6, Step 130, Loss 1.4168\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 6, Step 140, Loss 1.3786\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 6, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m [INFO] Patience 4 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 7, Step 0, Loss 1.3262\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 7, Step 10, Loss 1.3891\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 7, Step 20, Loss 1.3525\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 7, Step 30, Loss 1.3984\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 7, Step 40, Loss 1.3531\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 7, Step 50, Loss 1.3610\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 7, Step 60, Loss 1.3564\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 7, Step 70, Loss 1.3478\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 7, Step 80, Loss 1.4047\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 7, Step 90, Loss 1.4041\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 7, Step 100, Loss 1.4324\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 7, Step 110, Loss 1.3697\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 7, Step 120, Loss 1.3550\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 7, Step 130, Loss 1.3884\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 7, Step 140, Loss 1.3722\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 7, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m [INFO] Patience 3 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 8, Step 0, Loss 1.3797\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 8, Step 10, Loss 1.3979\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 8, Step 20, Loss 1.4129\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 8, Step 30, Loss 1.3711\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 8, Step 40, Loss 1.4030\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 8, Step 50, Loss 1.3604\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 8, Step 60, Loss 1.3779\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 8, Step 70, Loss 1.3932\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 8, Step 80, Loss 1.4022\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 8, Step 90, Loss 1.3747\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 8, Step 100, Loss 1.3713\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 8, Step 110, Loss 1.3981\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 8, Step 120, Loss 1.3899\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 8, Step 130, Loss 1.3539\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 8, Step 140, Loss 1.3634\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 00009: reducing learning rate of group 0 to 1.0000e-04.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 8, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m [INFO] Patience 2 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 9, Step 0, Loss 1.3548\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 9, Step 10, Loss 1.3507\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 9, Step 20, Loss 1.3986\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 9, Step 30, Loss 1.4310\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 9, Step 40, Loss 1.4007\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 9, Step 50, Loss 1.4081\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 9, Step 60, Loss 1.3317\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 9, Step 70, Loss 1.3955\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 9, Step 80, Loss 1.4253\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 9, Step 90, Loss 1.3945\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 9, Step 100, Loss 1.3527\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 9, Step 110, Loss 1.3703\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 9, Step 120, Loss 1.3919\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 9, Step 130, Loss 1.4249\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 9, Step 140, Loss 1.4606\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 9, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m [INFO] Patience 1 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 10, Step 0, Loss 1.4454\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 10, Step 10, Loss 1.3444\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 10, Step 20, Loss 1.3768\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 10, Step 30, Loss 1.3682\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 10, Step 40, Loss 1.3380\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 10, Step 50, Loss 1.3565\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 10, Step 60, Loss 1.3796\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 10, Step 70, Loss 1.3466\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 10, Step 80, Loss 1.3904\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 10, Step 90, Loss 1.4510\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 10, Step 100, Loss 1.3445\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 10, Step 110, Loss 1.4385\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 10, Step 120, Loss 1.4083\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 10, Step 130, Loss 1.3867\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 10, Step 140, Loss 1.3526\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m Epoch 10, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m [Early Stopping] Epoch 10, Valid Accuracy 19.84%, Valid Loss 1.3998\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2488)\u001b[0m [Client 2] get_parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=2775)\u001b[0m 2023-12-12 02:36:50,706\tERROR serialization.py:342 -- CUDA error: out of memory\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2775)\u001b[0m CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2775)\u001b[0m For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2775)\u001b[0m Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2775)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2775)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/ray/serialization.py\", line 340, in deserialize_objects\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2775)\u001b[0m     obj = self._deserialize_object(data, metadata, object_ref)\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2775)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/ray/serialization.py\", line 237, in _deserialize_object\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2775)\u001b[0m     return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2775)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/ray/serialization.py\", line 192, in _deserialize_msgpack_data\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2775)\u001b[0m     python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2775)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/ray/serialization.py\", line 182, in _deserialize_pickle5_data\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2775)\u001b[0m     obj = pickle.loads(in_band)\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2775)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/torch/storage.py\", line 241, in _load_from_bytes\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2775)\u001b[0m     return torch.load(io.BytesIO(b))\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2775)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2775)\u001b[0m     return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2775)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 1043, in _legacy_load\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2775)\u001b[0m     result = unpickler.load()\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2775)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 980, in persistent_load\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2775)\u001b[0m     wrap_storage=restore_location(obj, location),\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2775)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 217, in default_restore_location\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2775)\u001b[0m     result = fn(storage, location)\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2775)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 185, in _cuda_deserialize\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2775)\u001b[0m     return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2775)\u001b[0m RuntimeError: CUDA error: out of memory\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2775)\u001b[0m CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2775)\u001b[0m For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2775)\u001b[0m Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2775)\u001b[0m \n",
      "DEBUG flower 2023-12-12 02:36:51,330 | server.py:277 | fit_round received 1 results and 1 failures\n",
      "INFO flower 2023-12-12 02:37:10,689 | server.py:158 | fit progress: (6, 0.17673997841184103, {'val_accuracy': 0.25396825396825395, 'test_accuracy': 0.26540284360189575}, 3014.37218781)\n",
      "INFO flower 2023-12-12 02:37:10,690 | server.py:209 | evaluate_round: no clients selected, cancel\n",
      "DEBUG flower 2023-12-12 02:37:10,691 | server.py:265 | fit_round: strategy sampled 2 clients (out of 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m [Client 1] fit, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m /opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 0, Step 0, Loss 1.3877\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 0, Step 10, Loss 1.3342\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 0, Step 20, Loss 1.3617\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 0, Step 30, Loss 1.4383\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 0, Step 40, Loss 1.3543\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 0, Step 50, Loss 1.3772\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 0, Step 60, Loss 1.4358\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 0, Step 70, Loss 1.4361\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 0, Step 80, Loss 1.3964\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 0, Step 90, Loss 1.3742\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 0, Step 100, Loss 1.3456\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 0, Step 110, Loss 1.3785\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 0, Step 120, Loss 1.3934\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 0, Step 130, Loss 1.3996\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 0, Step 140, Loss 1.3414\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 0, Valid Accuracy 17.46%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 1, Step 0, Loss 1.3582\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 1, Step 10, Loss 1.3672\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 1, Step 20, Loss 1.3995\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 1, Step 30, Loss 1.3414\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 1, Step 40, Loss 1.3926\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 1, Step 50, Loss 1.4011\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 1, Step 60, Loss 1.3619\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 1, Step 70, Loss 1.3895\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 1, Step 80, Loss 1.3917\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 1, Step 90, Loss 1.4018\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 1, Step 100, Loss 1.3818\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 1, Step 110, Loss 1.3348\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 1, Step 120, Loss 1.4238\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 1, Step 130, Loss 1.4037\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 1, Step 140, Loss 1.3701\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 1, Valid Accuracy 17.46%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 2, Step 0, Loss 1.3764\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 2, Step 10, Loss 1.3818\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 2, Step 20, Loss 1.3943\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 2, Step 30, Loss 1.3947\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 2, Step 40, Loss 1.3827\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 2, Step 50, Loss 1.3640\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 2, Step 60, Loss 1.3759\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 2, Step 70, Loss 1.4007\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 2, Step 80, Loss 1.3742\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 2, Step 90, Loss 1.4014\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 2, Step 100, Loss 1.4139\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 2, Step 110, Loss 1.4272\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 2, Step 120, Loss 1.3587\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 2, Step 130, Loss 1.4132\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 2, Step 140, Loss 1.3974\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 2, Valid Accuracy 17.46%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 3, Step 0, Loss 1.3654\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 3, Step 10, Loss 1.4224\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 3, Step 20, Loss 1.3968\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 3, Step 30, Loss 1.3796\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 3, Step 40, Loss 1.4539\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 3, Step 50, Loss 1.3794\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 3, Step 60, Loss 1.3842\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 3, Step 70, Loss 1.3696\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 3, Step 80, Loss 1.3926\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 3, Step 90, Loss 1.3677\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 3, Step 100, Loss 1.3999\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 3, Step 110, Loss 1.4192\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 3, Step 120, Loss 1.3973\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 3, Step 130, Loss 1.3290\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 3, Step 140, Loss 1.3778\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 3, Valid Accuracy 17.46%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 4, Step 0, Loss 1.3834\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 4, Step 10, Loss 1.4157\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 4, Step 20, Loss 1.3765\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 4, Step 30, Loss 1.3663\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 4, Step 40, Loss 1.4341\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 4, Step 50, Loss 1.3129\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 4, Step 60, Loss 1.4277\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 4, Step 70, Loss 1.3679\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 4, Step 80, Loss 1.3385\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 4, Step 90, Loss 1.3979\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 4, Step 100, Loss 1.3725\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 4, Step 110, Loss 1.3723\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 4, Step 120, Loss 1.3686\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 4, Step 130, Loss 1.3690\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 4, Step 140, Loss 1.3778\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 4, Valid Accuracy 17.46%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m [INFO] Patience 5 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 5, Step 0, Loss 1.3715\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 5, Step 10, Loss 1.4034\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 5, Step 20, Loss 1.3901\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 5, Step 30, Loss 1.4089\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 5, Step 40, Loss 1.4142\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 5, Step 50, Loss 1.4050\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 5, Step 60, Loss 1.3684\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 5, Step 70, Loss 1.4048\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 5, Step 80, Loss 1.3946\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 5, Step 90, Loss 1.3433\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 5, Step 100, Loss 1.3819\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 5, Step 110, Loss 1.3907\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 5, Step 120, Loss 1.3986\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 5, Step 130, Loss 1.3991\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 5, Step 140, Loss 1.3610\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 5, Valid Accuracy 17.46%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m [INFO] Patience 4 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 6, Step 0, Loss 1.3680\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 6, Step 10, Loss 1.3698\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 6, Step 20, Loss 1.3993\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 6, Step 30, Loss 1.3662\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 6, Step 40, Loss 1.3574\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 6, Step 50, Loss 1.3701\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 6, Step 60, Loss 1.3758\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 6, Step 70, Loss 1.4023\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 6, Step 80, Loss 1.3517\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 6, Step 90, Loss 1.3908\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 6, Step 100, Loss 1.3914\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 6, Step 110, Loss 1.3580\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 6, Step 120, Loss 1.3950\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 6, Step 130, Loss 1.3330\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 6, Step 140, Loss 1.4146\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 6, Valid Accuracy 17.46%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 7, Step 0, Loss 1.3569\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 7, Step 10, Loss 1.3571\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 7, Step 20, Loss 1.3982\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 7, Step 30, Loss 1.3754\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 7, Step 40, Loss 1.3638\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 7, Step 50, Loss 1.3893\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 7, Step 60, Loss 1.4115\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 7, Step 70, Loss 1.3932\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 7, Step 80, Loss 1.3673\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 7, Step 90, Loss 1.3863\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 7, Step 100, Loss 1.3619\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 7, Step 110, Loss 1.3535\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 7, Step 120, Loss 1.3971\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 7, Step 130, Loss 1.4115\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 7, Step 140, Loss 1.4042\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 7, Valid Accuracy 26.98%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m [INFO] Patience 5 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 8, Step 0, Loss 1.3875\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 8, Step 10, Loss 1.4116\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 8, Step 20, Loss 1.3982\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 8, Step 30, Loss 1.3694\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 8, Step 40, Loss 1.3717\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 8, Step 50, Loss 1.3928\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 8, Step 60, Loss 1.3993\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 8, Step 70, Loss 1.3725\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 8, Step 80, Loss 1.3653\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 8, Step 90, Loss 1.4092\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 8, Step 100, Loss 1.3978\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 8, Step 110, Loss 1.3708\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 8, Step 120, Loss 1.3683\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 8, Step 130, Loss 1.3701\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 8, Step 140, Loss 1.3775\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 8, Valid Accuracy 17.46%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m [INFO] Patience 4 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 9, Step 0, Loss 1.4046\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 9, Step 10, Loss 1.3913\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 9, Step 20, Loss 1.3695\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 9, Step 30, Loss 1.3436\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 9, Step 40, Loss 1.3999\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 9, Step 50, Loss 1.3600\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 9, Step 60, Loss 1.3811\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 9, Step 70, Loss 1.3712\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 9, Step 80, Loss 1.3911\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 9, Step 90, Loss 1.3363\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 9, Step 100, Loss 1.4142\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 9, Step 110, Loss 1.4063\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 9, Step 120, Loss 1.3677\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 9, Step 130, Loss 1.3761\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 9, Step 140, Loss 1.3961\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 9, Valid Accuracy 17.46%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m [INFO] Patience 3 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 10, Step 0, Loss 1.4011\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 10, Step 10, Loss 1.4132\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 10, Step 20, Loss 1.3609\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 10, Step 30, Loss 1.4033\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 10, Step 40, Loss 1.3755\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 10, Step 50, Loss 1.4043\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 10, Step 60, Loss 1.3823\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 10, Step 70, Loss 1.4014\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 10, Step 80, Loss 1.4076\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 10, Step 90, Loss 1.3881\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 10, Step 100, Loss 1.4214\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 10, Step 110, Loss 1.3748\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 10, Step 120, Loss 1.3583\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 10, Step 130, Loss 1.3843\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 10, Step 140, Loss 1.3757\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 10, Valid Accuracy 17.46%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m [INFO] Patience 2 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 11, Step 0, Loss 1.3695\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 11, Step 10, Loss 1.4387\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 11, Step 20, Loss 1.4167\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 11, Step 30, Loss 1.3612\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 11, Step 40, Loss 1.3993\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 11, Step 50, Loss 1.3778\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 11, Step 60, Loss 1.3672\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 11, Step 70, Loss 1.3913\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 11, Step 80, Loss 1.4008\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 11, Step 90, Loss 1.3784\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 11, Step 100, Loss 1.3697\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 11, Step 110, Loss 1.3681\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 11, Step 120, Loss 1.4510\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 11, Step 130, Loss 1.3885\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 11, Step 140, Loss 1.3708\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 11, Valid Accuracy 17.46%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m [INFO] Patience 1 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 12, Step 0, Loss 1.4516\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 12, Step 10, Loss 1.3676\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 12, Step 20, Loss 1.3676\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 12, Step 30, Loss 1.3996\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 12, Step 40, Loss 1.3680\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 12, Step 50, Loss 1.4213\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 12, Step 60, Loss 1.3463\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 12, Step 70, Loss 1.3677\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 12, Step 80, Loss 1.4280\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 12, Step 90, Loss 1.3450\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 12, Step 100, Loss 1.4209\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 12, Step 110, Loss 1.4186\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 12, Step 120, Loss 1.3584\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 12, Step 130, Loss 1.3677\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 12, Step 140, Loss 1.3773\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m Epoch 12, Valid Accuracy 17.46%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m [Early Stopping] Epoch 12, Valid Accuracy 17.46%, Valid Loss 1.3954\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=2813)\u001b[0m [Client 1] get_parameters\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m [Client 2] fit, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m /opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 0, Step 0, Loss 1.3830\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 0, Step 10, Loss 1.3410\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 0, Step 20, Loss 1.3554\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 0, Step 30, Loss 1.3717\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 0, Step 40, Loss 1.3743\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 0, Step 50, Loss 1.4052\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 0, Step 60, Loss 1.3700\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 0, Step 70, Loss 1.3694\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 0, Step 80, Loss 1.3858\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 0, Step 90, Loss 1.3917\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 0, Step 100, Loss 1.3864\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 0, Step 110, Loss 1.3356\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 0, Step 120, Loss 1.4114\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 0, Step 130, Loss 1.3884\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 0, Step 140, Loss 1.4192\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 0, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 1, Step 0, Loss 1.3163\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 1, Step 10, Loss 1.3695\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 1, Step 20, Loss 1.4236\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 1, Step 30, Loss 1.3921\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 1, Step 40, Loss 1.3750\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 1, Step 50, Loss 1.3811\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 1, Step 60, Loss 1.3961\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 1, Step 70, Loss 1.3529\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 1, Step 80, Loss 1.3496\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 1, Step 90, Loss 1.3936\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 1, Step 100, Loss 1.3934\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 1, Step 110, Loss 1.4216\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 1, Step 120, Loss 1.3647\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 1, Step 130, Loss 1.4139\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 1, Step 140, Loss 1.3946\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 1, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 2, Step 0, Loss 1.3952\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 2, Step 10, Loss 1.3782\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 2, Step 20, Loss 1.3807\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 2, Step 30, Loss 1.3837\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 2, Step 40, Loss 1.4122\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 2, Step 50, Loss 1.4173\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 2, Step 60, Loss 1.3641\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 2, Step 70, Loss 1.3812\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 2, Step 80, Loss 1.4236\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 2, Step 90, Loss 1.3740\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 2, Step 100, Loss 1.3898\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 2, Step 110, Loss 1.3748\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 2, Step 120, Loss 1.3826\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 2, Step 130, Loss 1.4526\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 2, Step 140, Loss 1.4114\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 2, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m [INFO] Patience 5 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 3, Step 0, Loss 1.3644\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 3, Step 10, Loss 1.4006\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 3, Step 20, Loss 1.3610\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 3, Step 30, Loss 1.3720\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 3, Step 40, Loss 1.3726\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 3, Step 50, Loss 1.4096\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 3, Step 60, Loss 1.3839\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 3, Step 70, Loss 1.4087\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 3, Step 80, Loss 1.4155\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 3, Step 90, Loss 1.3865\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 3, Step 100, Loss 1.3445\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 3, Step 110, Loss 1.4028\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 3, Step 120, Loss 1.4004\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 3, Step 130, Loss 1.3830\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 3, Step 140, Loss 1.3763\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 3, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m [INFO] Patience 4 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 4, Step 0, Loss 1.4118\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 4, Step 10, Loss 1.4542\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 4, Step 20, Loss 1.3723\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 4, Step 30, Loss 1.3942\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 4, Step 40, Loss 1.4069\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 4, Step 50, Loss 1.3887\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 4, Step 60, Loss 1.3678\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 4, Step 70, Loss 1.4078\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 4, Step 80, Loss 1.4281\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 4, Step 90, Loss 1.3815\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 4, Step 100, Loss 1.3616\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 4, Step 110, Loss 1.3714\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 4, Step 120, Loss 1.3786\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 4, Step 130, Loss 1.4083\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 4, Step 140, Loss 1.3854\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 4, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m [INFO] Patience 3 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 5, Step 0, Loss 1.4176\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 5, Step 10, Loss 1.3576\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 5, Step 20, Loss 1.4197\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 5, Step 30, Loss 1.3546\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 5, Step 40, Loss 1.4104\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 5, Step 50, Loss 1.3428\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 5, Step 60, Loss 1.4230\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 5, Step 70, Loss 1.3942\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 5, Step 80, Loss 1.3732\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 5, Step 90, Loss 1.3119\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 5, Step 100, Loss 1.3603\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 5, Step 110, Loss 1.3447\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 5, Step 120, Loss 1.4570\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 5, Step 130, Loss 1.4091\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 5, Step 140, Loss 1.3549\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 00006: reducing learning rate of group 0 to 1.0000e-04.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 5, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m [INFO] Patience 2 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 6, Step 0, Loss 1.3449\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 6, Step 10, Loss 1.4047\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 6, Step 20, Loss 1.3742\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 6, Step 30, Loss 1.3525\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 6, Step 40, Loss 1.3703\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 6, Step 50, Loss 1.3789\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 6, Step 60, Loss 1.3806\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 6, Step 70, Loss 1.3496\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 6, Step 80, Loss 1.4199\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 6, Step 90, Loss 1.3720\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 6, Step 100, Loss 1.3857\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 6, Step 110, Loss 1.3984\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 6, Step 120, Loss 1.3227\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 6, Step 130, Loss 1.3993\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 6, Step 140, Loss 1.3449\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 6, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 7, Step 0, Loss 1.4065\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 7, Step 10, Loss 1.3867\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 7, Step 20, Loss 1.3782\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 7, Step 30, Loss 1.3809\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 7, Step 40, Loss 1.4364\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 7, Step 50, Loss 1.3766\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 7, Step 60, Loss 1.3987\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 7, Step 70, Loss 1.3202\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 7, Step 80, Loss 1.4185\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 7, Step 90, Loss 1.3847\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 7, Step 100, Loss 1.4459\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 7, Step 110, Loss 1.4073\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 7, Step 120, Loss 1.3906\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 7, Step 130, Loss 1.4350\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 7, Step 140, Loss 1.4007\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 7, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 8, Step 0, Loss 1.3618\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 8, Step 10, Loss 1.3877\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 8, Step 20, Loss 1.3878\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 8, Step 30, Loss 1.3851\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 8, Step 40, Loss 1.3888\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 8, Step 50, Loss 1.3780\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 8, Step 60, Loss 1.3970\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 8, Step 70, Loss 1.4183\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 8, Step 80, Loss 1.3786\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 8, Step 90, Loss 1.4170\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 8, Step 100, Loss 1.3614\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 8, Step 110, Loss 1.4201\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 8, Step 120, Loss 1.3684\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 8, Step 130, Loss 1.3725\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 8, Step 140, Loss 1.4030\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 8, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 9, Step 0, Loss 1.3940\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 9, Step 10, Loss 1.4063\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 9, Step 20, Loss 1.3886\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 9, Step 30, Loss 1.4081\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 9, Step 40, Loss 1.4347\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 9, Step 50, Loss 1.3603\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 9, Step 60, Loss 1.3731\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 9, Step 70, Loss 1.4015\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 9, Step 80, Loss 1.3433\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 9, Step 90, Loss 1.3904\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 9, Step 100, Loss 1.3902\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 9, Step 110, Loss 1.3611\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 9, Step 120, Loss 1.3914\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 9, Step 130, Loss 1.3715\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 9, Step 140, Loss 1.4181\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 9, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m [INFO] Patience 5 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 10, Step 0, Loss 1.3617\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 10, Step 10, Loss 1.3575\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 10, Step 20, Loss 1.4111\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 10, Step 30, Loss 1.3444\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 10, Step 40, Loss 1.3905\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 10, Step 50, Loss 1.3672\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 10, Step 60, Loss 1.4270\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 10, Step 70, Loss 1.3732\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 10, Step 80, Loss 1.3671\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 10, Step 90, Loss 1.4516\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 10, Step 100, Loss 1.4326\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 10, Step 110, Loss 1.3702\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 10, Step 120, Loss 1.3710\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 10, Step 130, Loss 1.3908\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 10, Step 140, Loss 1.3887\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 10, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m [INFO] Patience 4 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 11, Step 0, Loss 1.3292\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 11, Step 10, Loss 1.3843\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 11, Step 20, Loss 1.3902\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 11, Step 30, Loss 1.3726\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 11, Step 40, Loss 1.3552\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 11, Step 50, Loss 1.4054\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 11, Step 60, Loss 1.3823\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 11, Step 70, Loss 1.3620\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 11, Step 80, Loss 1.3659\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 11, Step 90, Loss 1.3722\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 11, Step 100, Loss 1.4003\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 11, Step 110, Loss 1.3815\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 11, Step 120, Loss 1.3848\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 11, Step 130, Loss 1.3543\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 11, Step 140, Loss 1.3885\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 11, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m [INFO] Patience 3 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 12, Step 0, Loss 1.4495\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 12, Step 10, Loss 1.4099\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 12, Step 20, Loss 1.3603\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 12, Step 30, Loss 1.3313\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 12, Step 40, Loss 1.3775\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 12, Step 50, Loss 1.3661\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 12, Step 60, Loss 1.4187\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 12, Step 70, Loss 1.4116\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 12, Step 80, Loss 1.3898\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 12, Step 90, Loss 1.3322\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 12, Step 100, Loss 1.3331\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 12, Step 110, Loss 1.4050\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 12, Step 120, Loss 1.3787\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 12, Step 130, Loss 1.4195\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 12, Step 140, Loss 1.3487\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 00013: reducing learning rate of group 0 to 1.0000e-05.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 12, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m [INFO] Patience 2 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 13, Step 0, Loss 1.3691\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 13, Step 10, Loss 1.3913\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 13, Step 20, Loss 1.3375\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 13, Step 30, Loss 1.4139\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 13, Step 40, Loss 1.4003\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 13, Step 50, Loss 1.3634\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 13, Step 60, Loss 1.3906\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 13, Step 70, Loss 1.4195\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 13, Step 80, Loss 1.3611\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 13, Step 90, Loss 1.3513\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 13, Step 100, Loss 1.3667\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 13, Step 110, Loss 1.3737\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 13, Step 120, Loss 1.4059\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 13, Step 130, Loss 1.3627\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 13, Step 140, Loss 1.3720\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 13, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 14, Step 0, Loss 1.3686\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 14, Step 10, Loss 1.3416\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 14, Step 20, Loss 1.4370\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 14, Step 30, Loss 1.3848\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 14, Step 40, Loss 1.3832\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 14, Step 50, Loss 1.3204\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 14, Step 60, Loss 1.4216\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 14, Step 70, Loss 1.3714\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 14, Step 80, Loss 1.4323\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 14, Step 90, Loss 1.4292\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 14, Step 100, Loss 1.3542\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 14, Step 110, Loss 1.4066\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 14, Step 120, Loss 1.3610\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 14, Step 130, Loss 1.3836\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 14, Step 140, Loss 1.3817\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 14, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m [INFO] Patience 5 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 15, Step 0, Loss 1.3546\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 15, Step 10, Loss 1.3839\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 15, Step 20, Loss 1.3481\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 15, Step 30, Loss 1.3830\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 15, Step 40, Loss 1.3607\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 15, Step 50, Loss 1.3682\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 15, Step 60, Loss 1.3646\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 15, Step 70, Loss 1.3442\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 15, Step 80, Loss 1.3538\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 15, Step 90, Loss 1.4347\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 15, Step 100, Loss 1.3683\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 15, Step 110, Loss 1.4085\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 15, Step 120, Loss 1.4028\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 15, Step 130, Loss 1.3924\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 15, Step 140, Loss 1.3777\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 15, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m [INFO] Patience 4 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 16, Step 0, Loss 1.3568\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 16, Step 10, Loss 1.3813\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 16, Step 20, Loss 1.3497\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 16, Step 30, Loss 1.3345\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 16, Step 40, Loss 1.4050\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 16, Step 50, Loss 1.4070\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 16, Step 60, Loss 1.3933\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 16, Step 70, Loss 1.3508\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 16, Step 80, Loss 1.3415\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 16, Step 90, Loss 1.3996\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 16, Step 100, Loss 1.3781\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 16, Step 110, Loss 1.3777\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 16, Step 120, Loss 1.3738\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 16, Step 130, Loss 1.3444\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 16, Step 140, Loss 1.4075\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 00017: reducing learning rate of group 0 to 1.0000e-06.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 16, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m [INFO] Patience 3 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 17, Step 0, Loss 1.3883\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 17, Step 10, Loss 1.3735\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 17, Step 20, Loss 1.3689\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 17, Step 30, Loss 1.3652\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 17, Step 40, Loss 1.3633\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 17, Step 50, Loss 1.4074\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 17, Step 60, Loss 1.4515\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 17, Step 70, Loss 1.3673\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 17, Step 80, Loss 1.3909\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 17, Step 90, Loss 1.4130\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 17, Step 100, Loss 1.3717\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 17, Step 110, Loss 1.3732\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 17, Step 120, Loss 1.3539\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 17, Step 130, Loss 1.3543\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 17, Step 140, Loss 1.3754\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 17, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m [INFO] Patience 2 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 18, Step 0, Loss 1.3949\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 18, Step 10, Loss 1.3647\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 18, Step 20, Loss 1.3374\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 18, Step 30, Loss 1.3625\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 18, Step 40, Loss 1.3304\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 18, Step 50, Loss 1.3874\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 18, Step 60, Loss 1.3888\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 18, Step 70, Loss 1.4141\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 18, Step 80, Loss 1.4248\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 18, Step 90, Loss 1.3615\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 18, Step 100, Loss 1.4182\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 18, Step 110, Loss 1.3780\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 18, Step 120, Loss 1.3901\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 18, Step 130, Loss 1.3762\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 18, Step 140, Loss 1.4657\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 18, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m [INFO] Patience 1 remaining\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 19, Step 0, Loss 1.4387\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 19, Step 10, Loss 1.3914\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 19, Step 20, Loss 1.3998\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 19, Step 30, Loss 1.4012\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 19, Step 40, Loss 1.4030\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 19, Step 50, Loss 1.3785\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 19, Step 60, Loss 1.3685\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 19, Step 70, Loss 1.3714\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 19, Step 80, Loss 1.3972\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 19, Step 90, Loss 1.3629\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 19, Step 100, Loss 1.4224\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 19, Step 110, Loss 1.3817\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 19, Step 120, Loss 1.3760\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 19, Step 130, Loss 1.3963\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 19, Step 140, Loss 1.3418\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m Epoch 19, Valid Accuracy 19.84%\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m [Early Stopping] Epoch 19, Valid Accuracy 19.84%, Valid Loss 1.3969\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=3144)\u001b[0m [Client 2] get_parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flower 2023-12-12 02:55:10,547 | server.py:277 | fit_round received 2 results and 0 failures\n",
      "INFO flower 2023-12-12 02:55:27,542 | server.py:158 | fit progress: (7, 0.1766035244578407, {'val_accuracy': 0.25396825396825395, 'test_accuracy': 0.26540284360189575}, 4111.225466399001)\n",
      "INFO flower 2023-12-12 02:55:27,543 | server.py:209 | evaluate_round: no clients selected, cancel\n",
      "INFO flower 2023-12-12 02:55:27,544 | server.py:182 | FL finished in 4111.227268839\n",
      "INFO flower 2023-12-12 02:55:27,625 | app.py:149 | app_fit: losses_distributed []\n",
      "INFO flower 2023-12-12 02:55:27,627 | app.py:150 | app_fit: metrics_distributed {}\n",
      "INFO flower 2023-12-12 02:55:27,628 | app.py:151 | app_fit: losses_centralized [(0, 0.1779284534000215), (1, 0.17626559355902294), (2, 0.17638013192585536), (3, 0.17622614474523635), (4, 0.17649534486588977), (5, 0.17634660005569458), (6, 0.17673997841184103), (7, 0.1766035244578407)]\n",
      "INFO flower 2023-12-12 02:55:27,628 | app.py:152 | app_fit: metrics_centralized {'val_accuracy': [(0, 0.25396825396825395), (1, 0.25396825396825395), (2, 0.25396825396825395), (3, 0.23015873015873015), (4, 0.25396825396825395), (5, 0.25396825396825395), (6, 0.25396825396825395), (7, 0.25396825396825395)], 'test_accuracy': [(0, 0.27014218009478674), (1, 0.26540284360189575), (2, 0.26540284360189575), (3, 0.26303317535545023), (4, 0.26540284360189575), (5, 0.26540284360189575), (6, 0.26540284360189575), (7, 0.26540284360189575)]}\n"
     ]
    }
   ],
   "source": [
    "model_params = get_parameters(model.to(DEVICE))\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "    fraction_fit=0.3,\n",
    "    fraction_eval=0.3,\n",
    "    min_fit_clients=2,\n",
    "    min_eval_clients=2,\n",
    "    min_available_clients=NUM_CLIENTS,\n",
    "    initial_parameters=fl.common.weights_to_parameters(get_parameters(model.to(DEVICE))),\n",
    "    eval_fn=get_evaluate_fn(),  # Pass the evaluation function\n",
    "    \n",
    ")\n",
    "\n",
    "#Start Simulation\n",
    "res = fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=NUM_CLIENTS,\n",
    "    num_rounds=7,\n",
    "    strategy=strategy,\n",
    "    client_resources = {'num_cpus': 4, 'num_gpus': 2}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d56646",
   "metadata": {
    "papermill": {
     "duration": 0.167536,
     "end_time": "2023-12-12T02:55:28.084360",
     "exception": false,
     "start_time": "2023-12-12T02:55:27.916824",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Plotting Grpahs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a75ddee6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T02:55:28.428181Z",
     "iopub.status.busy": "2023-12-12T02:55:28.427710Z",
     "iopub.status.idle": "2023-12-12T02:55:28.819743Z",
     "shell.execute_reply": "2023-12-12T02:55:28.818759Z"
    },
    "papermill": {
     "duration": 0.56439,
     "end_time": "2023-12-12T02:55:28.821693",
     "exception": false,
     "start_time": "2023-12-12T02:55:28.257303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rounds = [x for x in range(len(res.metrics_centralized['val_accuracy']))]\n",
    "val_accuracies = [x[1] for x in res.metrics_centralized['val_accuracy']]\n",
    "test_accuracies = [x[1] for x in res.metrics_centralized['test_accuracy']]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "# plt.ylim([-1,2])\n",
    "plt.plot(rounds, val_accuracies, 'b',marker=\"o\", label='Validation Accuracy')\n",
    "plt.plot(rounds, test_accuracies, 'r',marker=\"o\", label='Test Accuracy')\n",
    "# plt.plot(epochs, val_losses, 'r', label='Validation Loss')\n",
    "plt.title('Validation and Test Accuracy')\n",
    "plt.xlabel('Rounds')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb1a93cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T02:55:29.160359Z",
     "iopub.status.busy": "2023-12-12T02:55:29.160012Z",
     "iopub.status.idle": "2023-12-12T02:55:29.484364Z",
     "shell.execute_reply": "2023-12-12T02:55:29.483451Z"
    },
    "papermill": {
     "duration": 0.494767,
     "end_time": "2023-12-12T02:55:29.486407",
     "exception": false,
     "start_time": "2023-12-12T02:55:28.991640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rounds = range(0, len(res.losses_centralized))\n",
    "val_losses = [x[1] for x in res.losses_centralized]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "# plt.ylim([-1,2])\n",
    "plt.plot(rounds, val_losses, 'b', marker=\"o\", label='Validation Loss')\n",
    "# plt.plot(epochs, val_losses, 'r', label='Validation Loss')\n",
    "plt.title('Validation Loss')\n",
    "plt.xlabel('Rounds')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24239d92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T02:55:29.836928Z",
     "iopub.status.busy": "2023-12-12T02:55:29.836540Z",
     "iopub.status.idle": "2023-12-12T02:55:29.842643Z",
     "shell.execute_reply": "2023-12-12T02:55:29.841749Z"
    },
    "papermill": {
     "duration": 0.18657,
     "end_time": "2023-12-12T02:55:29.845260",
     "exception": false,
     "start_time": "2023-12-12T02:55:29.658690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "History (loss, centralized):\n",
       "\tround 0: 0.1779284534000215\n",
       "\tround 1: 0.17626559355902294\n",
       "\tround 2: 0.17638013192585536\n",
       "\tround 3: 0.17622614474523635\n",
       "\tround 4: 0.17649534486588977\n",
       "\tround 5: 0.17634660005569458\n",
       "\tround 6: 0.17673997841184103\n",
       "\tround 7: 0.1766035244578407\n",
       "History (metrics, centralized):\n",
       "{'val_accuracy': [(0, 0.25396825396825395), (1, 0.25396825396825395), (2, 0.25396825396825395), (3, 0.23015873015873015), (4, 0.25396825396825395), (5, 0.25396825396825395), (6, 0.25396825396825395), (7, 0.25396825396825395)], 'test_accuracy': [(0, 0.27014218009478674), (1, 0.26540284360189575), (2, 0.26540284360189575), (3, 0.26303317535545023), (4, 0.26540284360189575), (5, 0.26540284360189575), (6, 0.26540284360189575), (7, 0.26540284360189575)]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2440665,
     "sourceId": 4130910,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30616,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4191.840848,
   "end_time": "2023-12-12T02:55:35.765474",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-12T01:45:43.924626",
   "version": "2.4.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "fa24e9454f98d8d2dd8b168680dec3ce900d4efa54ef106babd2dbc69ffdb00e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
